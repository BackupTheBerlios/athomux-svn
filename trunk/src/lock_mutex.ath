Author: Roland Niese
Copyright: Roland Niese
License: see files SOFTWARE-LICENSE, PATENT-LICENSE

//context pconf: *
//context cconf: *

brick #lock_mutex

purpose Mandate-independent locks

desc
* Read locks conflict with write locks only.
* Write locks conflict with any locks.
* granularity is 1.
* A lock conflict causes the later requesting operation to either be blocked or to fail, depending on the action parameter.
* Ignores mandates (conflicting lock calls with the same mandate block!)
* No accumulating locks (impossible without operator identification), use lock_mand for this.
* Non-conflicting, but overlapping locks stack. The sum of $lock and $unlock must be 0.
* Data and address locks are independent from each other.
* Brick is implemented statelessly. Needs an exclusive nest at :<tmp with nest semantics.
* Brick is system-independent. Uses #lock_native for low-level locks. Adapt that brick for new runtime environments.
* FIXME: attempts to lock the highest byte (2^64-1) of the nest will fail due to integer overflow!

      +--------+
tmp---+        |
in----+        +---out
      |        |
      +--------+  
enddesc

static_data {
	@.define MAX_LOCK_RECORDS (1024)
	@.define STATE_STORAGE_SIZE (sizeof (struct brick_state))
	
	// Abstract reference type (was necessary to ease conversion from internal vars to state nest).
	typedef addr_t *lock_ref_t;
	// Essential properties of a lock.
	struct lock_desc {
		addr_t start;
		addr_t end;
		unsigned short flags;
	};
	// Record in a doubly linked list of locks.
	// Also contains implementation of thread blocking.
	struct lock_rec {
		lock_ref_t next_start;
		lock_ref_t prev_start;
		// Signal broadcasted when a locked block is [partially] unlocked.
		addr_t native_lock;
		struct lock_desc desc;
	};
	// Structure of :<tmp, from address 0.
	struct brick_state {
		lock_ref_t lock_starting_first;
		lock_ref_t lock_starting_last;
		lock_ref_t free_lock_recs;
		struct lock_rec lock_rec_storage[MAX_LOCK_RECORDS];
	};

	// Filter for locks in the active list.
	typedef bool (*lock_mutex_lock_test_t) (struct brick_state *state, lock_ref_t lock, struct lock_desc *desc);
	// 'Primes' a lock: initialize the native lock mechanism.
	@.define activate_lock (LOCK) {
		success_t ok;
		@=outputcall ##lock_imp:>out$gadr (1) => (ok, MAKE_STATE_PTR (LOCK)->native_lock);
		@.check (!ok, "Failed to obtain native lock!");
	}
	
	// Releases all threads waiting for this lock, then deactivates the native lock mechanism.
	@.define deactivate_lock (LOCK) {
		success_t ok;
		@=outputcall ##lock_imp:>out$padr (MAKE_STATE_PTR (LOCK)->native_lock, 1) => (ok);
		@.check (!ok, "Failed to drop native lock!");
	}

	// Makes a thread wait for removal of this lock.
	@.define wait_for_lock (LOCK) {
		success_t ok;
		@=outputcall ##lock_imp:>out$lock (MAKE_STATE_PTR (LOCK)->native_lock, 1) => (ok);
		@.check (!ok, "Failed to wait on native lock!");
	}
	
	// Reserve exclusive access to the lock list.
	@.define lock_mutex () {
		success_t ok;
		@=outputcall ##lock_imp:>out$lock ((addr_t)-1, 1) => (ok);
		@.check (!ok, "Failed to reserve native mutex!");
	}
	
	// Release exclusive access to the lock list.
	@.define unlock_mutex () {
		success_t ok;
		@=outputcall ##lock_imp:>out$unlock ((addr_t)-1, 1) => (ok);
		@.check (!ok, "Failed to release native mutex!");
	}

	@.define MAX_LOCK_RECORDS (1024)

	// Abstract reference values/querys/operations.
	@.define NULL_REF ((lock_ref_t)0)
	
	// A valid physical pointer "state" to the brick state must be declared for these macros to work.
	@.define MAKE_STATE_PTR (REF) ((struct lock_rec *)((void *)((paddr_t)(state) + (plen_t)(REF))))
	@.define MAKE_STATE_REF (PTR) ((lock_ref_t)((char *)(PTR) - (char *)state))
	
	// Abstract operations on the lock list.
	@.define has_next_lock (REF) (MAKE_STATE_PTR (REF)->next_start != 0)
	@.define has_prev_lock (REF) (MAKE_STATE_PTR (REF)->prev_start != 0)
	@.define get_next_lock (REF) (MAKE_STATE_PTR (REF)->next_start)
	@.define get_prev_lock (REF) (MAKE_STATE_PTR (REF)->prev_start)
	@.define set_next_lock (REF, NEXT_REF) (MAKE_STATE_PTR (REF)->next_start = (NEXT_REF))
	@.define set_prev_lock (REF, PREV_REF) (MAKE_STATE_PTR (REF)->prev_start = (PREV_REF))

	@.define make_lock_type (DATA_LOCK, ADDR_LOCK) (((ADDR_LOCK) << 2) | (DATA_LOCK))

	@.define get_desc_start (DESC) ((DESC).start)
	@.define get_desc_end (DESC) ((DESC).end)
	@.define get_desc_type (DESC) ((DESC).flags & 0x000F)
	@.define set_desc_start (DESC, ADDR) ((DESC).start = (ADDR))
	@.define set_desc_end (DESC, ADDR) ((DESC).end = (ADDR))
	@.define set_desc_bounds (DESC, START_ADDR, END_ADDR) (set_desc_start (DESC, START_ADDR), set_desc_end (DESC, END_ADDR))
	@.define set_desc (DESC, ADDR_START, ADDR_END, TYPE) ((DESC).start = ADDR_START, (DESC).end = ADDR_END, (DESC).flags = TYPE)
	
	@.define get_lock_start (REF) (get_desc_start (MAKE_STATE_PTR (REF)->desc))
	@.define get_lock_end (REF) (get_desc_end (MAKE_STATE_PTR (REF)->desc))
	@.define get_lock_data_type (REF) (MAKE_STATE_PTR (REF)->desc.flags & 0x0003)
	@.define get_lock_addr_type (REF) ((MAKE_STATE_PTR (REF)->desc.flags & 0x000C) >> 2)
	@.define get_lock_type (REF) (get_desc_type (MAKE_STATE_PTR (REF)->desc))
	@.define get_lock_desc (REF) (MAKE_STATE_PTR (REF)->desc)
	@.define set_lock_start (REF, ADDR) (set_desc_start (MAKE_STATE_PTR (REF)->desc, ADDR))
	@.define set_lock_end (REF, ADDR) (set_desc_end (MAKE_STATE_PTR (REF)->desc, ADDR))
	@.define set_lock_bounds (REF, START_ADDR, END_ADDR) (set_desc_bounds (MAKE_STATE_PTR (REF)->desc, START_ADDR, END_ADDR))
	@.define set_lock_type (REF, LOCK_TYPE) (MAKE_STATE_PTR (REF)->desc.flags = ((MAKE_STATE_PTR (REF)->desc.flags & ~0x000F) | (LOCK_TYPE)))
	@.define set_lock_desc (REF, DESC) (MAKE_STATE_PTR (REF)->desc = (DESC))

	// lock_type = (addr_lock * 4 + data_lock)
	// conflict on (lock_conflict_def[lock_type(A)] & (1 << lock_type(B))) != 0
	static unsigned short lock_conflict_def[] = {
		0x000, 0x444, 0x666, 0x000,
		0x700, 0x744, 0x766, 0x000,
		0x770, 0x774, 0x776
	};
	
	// Tests a lock record in a list for conflict with a given lock descriptor.
	// Assumption is that the two lock records overlap.
	bool is_conflict(struct brick_state *state, lock_ref_t lock, struct lock_desc *desc) {
		return (lock_conflict_def[get_desc_type (*desc)] & (1 << get_lock_type (lock))) != 0;
	}
	
	// Tests a lock record in a list, if it is 'owned' (was set by) the same individual owning desc.
	// Assumption is that the two lock records overlap.
	bool is_own (struct brick_state *state, lock_ref_t lock, struct lock_desc *desc) {
		return TRUE;
	}
}

instance #lock_native as lock_imp;


	// Allocates a new lock from the storage.
	@.define new_lock_rec (STATE_PTR) => (LOCK) {
		PC_DIRTY (tmp_state, 0);
		trace ("mand=%d allocate new lock record...\n", (int)@mandate);
		LOCK = (STATE_PTR)->free_lock_recs;
		@.fatal (!LOCK, "No more free lock records!");
		(STATE_PTR)->free_lock_recs = get_next_lock (LOCK);
		@.if ("PCONF" =~ m/debug$/) {
			set_next_lock (LOCK, NULL_REF);
			set_prev_lock (LOCK, NULL_REF);
			set_lock_bounds(LOCK, 0, 0);
		}
	}
	
	// Returns a lock record to the list of free records.
	// The record must have been removed from the active list (via remove_lock_rec) before!
	@.define free_lock_rec (STATE_PTR, LOCK) {
		PC_DIRTY (tmp_state, 0);
		trace ("mand=%d free lock [0x%llX..0x%llX]...\n", (int)@mandate, get_lock_start (LOCK), get_lock_end (LOCK));
		set_next_lock (LOCK, (STATE_PTR)->free_lock_recs);
		set_prev_lock (LOCK, NULL_REF);
		(STATE_PTR)->free_lock_recs = LOCK;
	}

	// Activates a lock record and adds it to the list.
	@.func add_lock_rec (struct brick_state *state, lock_ref_t lock) {
		lock_ref_t scan;
		PC_DIRTY (tmp_state, 0);
		trace ("mand=%d activate lock [0x%llX..0x%llX]...\n", (int)@mandate, get_lock_start (lock), get_lock_end (lock));
		for (scan = state->lock_starting_first; scan != NULL_REF && get_lock_start (scan) < get_lock_start (lock); scan = get_next_lock (scan));
		activate_lock (lock);
		set_next_lock (lock, scan);
		if (get_next_lock (lock) != NULL_REF) {
			lock_ref_t next_lock = get_next_lock (lock);
			lock_ref_t prev_lock = get_prev_lock (next_lock);
			set_prev_lock (lock, prev_lock);
			set_prev_lock (next_lock, lock);
		}
		else {
			set_prev_lock (lock, state->lock_starting_last);
			state->lock_starting_last = lock;
		}
		if (get_prev_lock (lock) != NULL_REF) {
			lock_ref_t prev_lock = get_prev_lock (lock);
			set_next_lock (prev_lock, lock);
		}
		else {
			state->lock_starting_first = lock;
		}		
	}
	
	// Removes a lock record from the list and releases all threads waiting for it.
	@.func remove_lock_rec (struct brick_state *state, lock_ref_t lock) {
		PC_DIRTY (tmp_state, 0);
		trace ("mand=%d deactivate lock [0x%llX..0x%llX]...\n", (int)@mandate, get_lock_start (lock), get_lock_end (lock));
		if (get_prev_lock (lock) != NULL_REF) {
			lock_ref_t prev_lock = get_prev_lock (lock);
			lock_ref_t next_lock = get_next_lock (lock);
			set_next_lock (prev_lock, next_lock);
		}
		else {
			state->lock_starting_first = get_next_lock (lock);
		}
		if (get_next_lock (lock) != NULL_REF) {
			lock_ref_t next_lock = get_next_lock (lock);
			lock_ref_t prev_lock = get_prev_lock (lock);
			set_prev_lock (next_lock, prev_lock);
		}
		else {
			state->lock_starting_last = get_prev_lock (lock);
		}
		deactivate_lock (lock);
	}

	// Allocates a new lock 'COPY' from the storage and copies
	// LOCK's essential characteristics to COPY. List and condition variables
	// are not copied.
	@.define copy_lock_rec (STATE, LOCK) => (COPY) {
		new_lock_rec (STATE) => (COPY);
		@.check ((COPY) == NULL_REF, "Out of lock records!");
		set_lock_desc (COPY, get_lock_desc (LOCK));
	}
	
	// Returns the first (=lowest start address) lock record in the list that
	// overlaps the specified descriptor and fulfills the specified requirement.
	@.define find_first_lock (STATE_PTR, DESC_PTR, REQUIREMENT) => (RESULT) {
		find_next_lock_from (STATE_PTR, (STATE_PTR)->lock_starting_first, DESC_PTR, REQUIREMENT) => (RESULT);
	}
	
	// Returns the next (=next-higher start address from 'lock') lock record in the list that
	// overlaps the specified descriptor and fulfills the specified requirement.
	@.func find_next_lock_from (struct brick_state *state, lock_ref_t lock, struct lock_desc *desc, lock_mutex_lock_test_t requirement) => (lock_ref_t result) {
		for (result = lock; result != NULL_REF; result = get_next_lock (result)) {
			if (get_lock_end (result) > get_desc_start (*desc)) {
				if (get_lock_start (result) >= get_desc_end (*desc)) {
					result = NULL_REF;
					break;
				}
				if (requirement (state, result, desc)) {
					break;
				}
			}
		}
		if (result != NULL_REF) {
			trace ("mand=%d Found lock [0x%llX..0x%llX] in region [0x%llX..0x%llX].\n", (int)@mandate, get_lock_start (result), get_lock_end (result), get_desc_start (*desc), get_desc_end (*desc));
		}
		else {
			trace ("mand=%d Found no lock in region [0x%llX..0x%llX].\n", (int)@mandate, get_desc_start (*desc), get_desc_end (*desc));
		}
	}
	
	@.if ("PCONF" =~ m/debug$/) {
		// Prints out the lock list.
		@.func print_lock_list (struct brick_state *state) {
			char buf[1024];
			int n;
			char *buf_end;
			lock_ref_t lock, prev_lock;
			lock = state->lock_starting_first;
			prev_lock = NULL_REF;
			trace ("mand=%d lock list:\n", (int)@mandate);
			memset(buf, 0, sizeof buf);
			n = sizeof buf - 2;
			buf_end = buf;
			while (lock != NULL_REF && n > 0) {
				int len;
				snprintf(buf_end, n, "[0x%llX..0x%llX] ", get_lock_start (lock), get_lock_end (lock));
				len = strlen(buf_end);
				n -= len;
				buf_end += len;
				if (get_prev_lock (lock) != prev_lock) {
					trace ("mand=%d %s <-- INCONSISTENCY!!!\n", (int)@mandate, buf);
					return;
				}
				prev_lock = lock;
				lock = get_next_lock (lock);
			}
			if (state->lock_starting_last != prev_lock) {
				trace ("mand=%d %s <-- FINAL INCONSISTENCY!!!\n", (int)@mandate, buf);
			}
			trace ("mand=%d %s (ok)\n", (int)@mandate, buf);
		}
	}
input :<in

	// Route $retract calls to :>out. The underlying nest might need retraction of other resources than locks.
	operation $retract {
		@=inputcall :>out$retract @args;
	}

input :<tmp

	use PC tmp_state [4] ;

output :>out
	
	// Initializes/destroys status variables.
	operation $output_init {
		success_t ok;

		@=outputcall ##lock_imp:>out$output_init (@destr, @constr) => (ok);
		@.check (!ok, "##lock_imp:>out$output_init failed!");
		PC_FLUSH (tmp_state);
		if (@destr) {
			@=outputcall :<tmp$delete (0, STATE_STORAGE_SIZE) => (ok);
			@.check (!ok, "Could not destroy state storage!");
		}
		if (@constr) {
			struct brick_state *state;
			int i;
			@=outputcall :<tmp$create (0, STATE_STORAGE_SIZE, TRUE) => (ok);
			state = PC_GET_DIRTY (tmp_state, 0, STATE_STORAGE_SIZE);
			@.check (!ok || !state, "Could not get state storage!");
			for (i = 0; i < MAX_LOCK_RECORDS - 1; i++) {
				state->lock_rec_storage[i].next_start = MAKE_STATE_REF (&state->lock_rec_storage[i + 1]);
			}
			state->lock_rec_storage[i].next_start = NULL_REF;
			state->lock_starting_first = NULL_REF;
			state->lock_starting_last = NULL_REF;
			state->free_lock_recs = MAKE_STATE_REF (state->lock_rec_storage);
		}
		@success = TRUE;
	}

	// See description in brick header.
	operation $lock {
		addr_t obl_start = @log_addr;
		addr_t obl_end = @log_addr + @log_len;
		addr_t opt_start = @try_addr;
		addr_t opt_end = @try_addr + @try_len;
		struct brick_state *state;
		struct lock_desc desc;
		lock_ref_t conflictor;
		@.check (opt_end < opt_start || obl_end < obl_start, "Lock  [0x%llX..[0x%llX..0x%llX]..0x%llX] exceeds address space!", opt_start, obl_start, obl_end, opt_end);
		@.check (opt_start > obl_start || opt_end < obl_end, "Optional part [0x%llX..0x%llX] does not contain obligatory part [0x%llX..0x%llX]!", opt_start, opt_end, obl_start, obl_end);
		
		if (opt_end - opt_start == 0 || (@data_lock == lock_none && @addr_lock == lock_none)) {
			// Nothing to lock, return.
			trace ("mand=%d Requested lock [?0x%llX..[!0x%llX..0x%llX!]..0x%llX?] has no effect.\n", (int)@mandate, opt_start, obl_start, obl_end, opt_end);
			@success = TRUE;
			return;
		}
		
		set_desc (desc, obl_start, obl_end, make_lock_type (@data_lock, @addr_lock));

		lock_mutex ();		
		
		state = PC_GET (tmp_state, 0, STATE_STORAGE_SIZE);
		@.check (!state, "Error getting brick status!");
		
		// Look for locks conflicting with the obligatory part of our requested lock.
		find_first_lock (state, &desc, is_conflict) => (conflictor);
		if (obl_end - obl_start > 0) {
			if (conflictor != NULL_REF) {
				if (@action != action_wait) {
					unlock_mutex ();
					return;
				}
				do {
					// Wait for the lock to be released.
					wait_for_lock (conflictor);
					// Start all over again, there could be more locks in the way/ new locks may have appeared.
					find_first_lock (state, &desc, is_conflict) => (conflictor);
				} while (conflictor != NULL_REF);
			}
		}
		else {
			if (conflictor != NULL_REF) {
				// This is the very special case of a zero-length obligatory part truly inside a conflicting lock,
				// What results in a successful lock of zero bytes.
				// (Code below relies on no locks across the whole obligatory part)
				@try_addr = @log_addr;
				@try_len = 0;
				@success = TRUE;
				trace ("Address 0x%llX truly contained inside lock [0x%llX..0x%llX], \"locked\" 0 bytes.\n", obl_start, get_lock_start (conflictor), get_lock_end (conflictor));
				unlock_mutex ();
				return;
			}
		}
		
		// Cut the optional parts until nothing conflicts with them anymore.
		if (opt_start < obl_start || opt_end > obl_end) {
			set_desc_bounds (desc, opt_start, opt_end);
			find_first_lock (state, &desc, is_conflict) => (conflictor);
			while (conflictor != NULL_REF) {
				if (get_lock_end (conflictor) <= obl_start) {
					// Conflicting lock is below the obligatory part, cut optional part upwards.
					set_desc_start (desc, get_lock_end (conflictor));
				}
				else {
					// Conflicting is above the obligatory part, cut optional part downwards.
					set_desc_end (desc, get_lock_start (conflictor));
				}
				find_next_lock_from (state, conflictor, &desc, is_conflict) => (conflictor);
			}
		}
		
		// Now the way is free: insert a new lock record, if appropriate.
		if (get_desc_end (desc) > get_desc_start (desc) && @action != action_ask) {
			lock_ref_t lock;
			new_lock_rec (state) => (lock);
			set_lock_desc (lock, desc);
			add_lock_rec (state, lock);
		}

		trace ("mand=%d Lock [0x%llX..0x%llX] granted.\n", (int)@mandate, get_desc_start (desc), get_desc_end (desc));
		@.if ("PCONF" =~ m/debug$/) {
			print_lock_list (state);
		}
		unlock_mutex ();
		
		@try_addr = get_desc_start (desc);
		@try_len = get_desc_end (desc) - get_desc_start (desc);
		@success = TRUE;
	}
	
	// See description in brick header.
	operation $unlock {
		lock_ref_t lock;
		struct lock_desc desc;
		struct brick_state *state;
		// Unlocking will always succeed, so we unlock the whole optional part right away.
		set_desc (desc, @try_addr, @try_addr + @try_len, make_lock_type (lock_write, lock_write));
		@.check (get_desc_start (desc) > @log_addr || get_desc_end (desc) < @log_addr + @log_len, "Optional part [0x%llX..0x%llX] does not contain obligatory part [0x%llX..0x%llX]!", get_desc_start (desc), get_desc_end (desc), @log_addr, @log_addr + @log_len);

		lock_mutex ();
		trace ("mand=%d Unlock of [0x%llX..0x%llX] requested.\n", (int)@mandate, get_desc_start (desc), get_desc_end (desc));
		
		state = PC_GET (tmp_state, 0, STATE_STORAGE_SIZE);
		@.check (!state, "Error getting brick status!");
		
		find_first_lock (state, &desc, is_own) => (lock);
		while (lock != NULL_REF) {
			lock_ref_t next_lock = get_next_lock (lock);
			// If the block is not completely covered by this $unlock, it must be split.
			// Create the (potential) lower and the upper leftovers as separate new locks.
			// Adjust start of 'desc' so that only one lock per address gets removed.
			if (get_lock_start (lock) < get_desc_start (desc)) {
				lock_ref_t lower_part;
				copy_lock_rec (state, lock) => (lower_part);
				set_lock_end (lower_part, get_desc_start (desc));
				add_lock_rec (state, lower_part);
			}
			if (get_lock_end (lock) > get_desc_end (desc)) {
				lock_ref_t upper_part;
				copy_lock_rec (state, lock) => (upper_part);
				set_lock_start (upper_part, get_desc_end (desc));
				add_lock_rec (state, upper_part);
				set_desc_start (desc, get_desc_end (desc));
			}
			else {
				set_desc_start (desc, get_lock_end (lock));
			}
			// Now remove the original lock. 
			remove_lock_rec (state, lock);
			free_lock_rec (state, lock);
			if (get_desc_start (desc) == get_desc_end (desc)) {
				// Any more overlapping locks don't need to be touched.
				break;
			}
			find_next_lock_from (state, next_lock, &desc, is_own) => (lock);
		}
		trace ("mand=%d Lock [0x%llX..0x%llX] released.\n", (int)@mandate, @try_addr, @try_addr + @try_len);
		@.if ("PCONF" =~ m/debug$/) {
			print_lock_list (state);
		}
		unlock_mutex ();
		@success = TRUE;
	}

	// Forward any other operations to :<in.
	operation $op {
		@=outputcall :<in$op @args;
	}
