Author: Roland Niese
Copyright: Roland Niese
License: see files SOFTWARE-LICENSE, PATENT-LICENSE

brick #lock_mutex_ulinux

purpose Mandate-independent locks

desc
* Implements simple locking for local use: bricks can use it e.g. to protect their status nest against concurrent control flows.
* Ignores lock types and mandates (_all_ overlapping lock requests conflict).
* Does not issue retracts.
* Dangerous when used outside a hosting brick: even the same thread can deadlock itself by issuing two overlapping locks (no accumulation)!
* Thus no overlapping locks allowed, no lock accumulation possible (impossible anyway without mandates).
* No overlapping locks allowed (would cause blocking), even if not conflicting (e.g. two data read locks, or one data and one address lock).
* All ops except $lock and $unlock on the output are forwarded to the input.
* Incoming $retract's are forwarded to the output, and ignored otherwise.
* BUG LEFT: pthread_cond_destroy sometimes blocks immediately after a pthread_cond_broadcast (why?)

      +--------+
      |        |
in ---+        +--- out
      |        |
      +--------+  
enddesc

static_header {
	#ifndef __LOCK_BRICK_STUFF__
	#define __LOCK_BRICK_STUFF__
	
	#include <pthread.h>

	@.define LOCK_STORAGE_SIZE (1024)

	// Abstract reference type to ease later conversion to Pointer Cache.
	typedef struct lock_rec *ref_t;
	// Record describing a locked region of a nest.
	struct lock_rec {
		ref_t next_start;
		ref_t prev_start;
		addr_t start;
		addr_t end;
		// Signal broadcasted when a locked block is [partially] unlocked.
		pthread_cond_t unlocked;
	};
	#endif
}

input :<in

	// Route $retract calls to :>out. The underlying nest might need retraction of other resources than locks.
	operation $retract {
		@=inputcall :>out$retract @args;
	}

output :>out

	data {
		// Storage for doubly linked list of locks. Could (should) later be stored in an input nest nest instead.
		struct lock_rec lock_rec_storage[LOCK_STORAGE_SIZE];
		// Reference to the lock with the lowest address.
		ref_t lock_starting_first;
		ref_t lock_starting_last;
		// Reference to the start of the list of unallocated blocks (first element has no 'previous' element).
		ref_t free_lock_recs;
// FIXME: this 'trash' should not be necessary! I use this to keep a reference to all used-up lock records that
// I cannot reuse as long as I cannot destroy their pthread_cond variable.
		ref_t lock_trash;
		// Mutex to protect lock list from concurrent use. */
		pthread_mutex_t lock_list_access;
	}

	// Abstract reference values/querys/operations.
	@.define NULL_REF ((ref_t)NULL)
	
	// Abstract operations on the lock list.
	@.define has_next (HERE) (HERE->next_start != 0)
	@.define has_prev (HERE) (HERE->prev_end != 0)
	@.define next_start_from (HERE) (HERE->next_start)
	@.define prev_start_from (HERE) (HERE->prev_start)
	@.define next_end_from (HERE) (HERE->next_end)
	@.define prev_end_from (HERE) (HERE->prev_end)
	
	// Allocates a new lock from the storage. Lock is not initialized anyhow.
	@.define new_lock_rec () => (LOCK) {
		trace ("mand=%d allocate new lock record...\n", (int)@mandate);
		LOCK = @:>.free_lock_recs;
		@.fatal (!LOCK, "No more lock records!");
		@:>.free_lock_recs = LOCK->next_start;
	}
	
	// Returns a lock record to the list of free records.
	// The record is not touched otherwise! It must not be part of the list!
	@.define free_lock_rec (LOCK) {
		trace ("mand=%d free lock [0x%llX..0x%llX]...\n", (int)@mandate, LOCK->start, LOCK->end);
// FIXME: freed locks should be returned to the free_lock_recs instead of this silly trash.
		LOCK->next_start = @:>.lock_trash;
		LOCK->prev_start = NULL_REF;
		@:>.lock_trash = LOCK;
	}

	// Activates (creates condition) a lock record and adds it to the list.
	@.func add_lock_rec (ref_t lock) {
		ref_t scan;
		trace ("mand=%d activate lock [0x%llX..0x%llX]...\n", (int)@mandate, lock->start, lock->end);
		for (scan = @:>.lock_starting_first; scan != NULL_REF && scan->start < lock->start; scan = scan->next_start);
		@.fatal (pthread_cond_init(&lock->unlocked, NULL) != 0, "Could not create POSIX lock!");
		lock->next_start = scan;
		if (lock->next_start != NULL_REF) {
			lock->prev_start = lock->next_start->prev_start;
			lock->next_start->prev_start = lock;
		}
		else {
			lock->prev_start = @:>.lock_starting_last;
			@:>.lock_starting_last = lock;
		}
		if (lock->prev_start != NULL_REF) {
			lock->prev_start->next_start = lock;
		}
		else {
			@:>.lock_starting_first = lock;
		}		
	}
	
	// Deactivates (signals and destroys condition) and removes a lock record from the list.
	@.func remove_lock_rec (ref_t lock) {
		trace ("mand=%d deactivate lock [0x%llX..0x%llX]...\n", (int)@mandate, lock->start, lock->end);
		if (lock->prev_start != NULL_REF) {
			lock->prev_start->next_start = lock->next_start;
		}
		else {
			@:>.lock_starting_first = lock->next_start;
		}
		if (lock->next_start != NULL_REF) {
			lock->next_start->prev_start = lock->prev_start;
		}
		else {
			@:>.lock_starting_last = lock->prev_start;
		}
		trace ("mand=%d broadcast unlock...\n", (int)@mandate);
		@.fatal (pthread_cond_broadcast(&lock->unlocked) != 0, "Could not release POSIX lock!");
		trace ("mand=%d destroy condition variable...\n", (int)@mandate);
// FIXME: this call seems to block when multiple threads were waiting for the signal just broadcasted,
// although a broadcast should make all of them wait for the mutex instead.
// Having an increasing number of undestroyed pthread_conds doesn't seem to me like a wise idea either.
//		@.fatal (pthread_cond_destroy(&lock->unlocked) != 0, "Could not destroy POSIX lock!");
		trace ("mand=%d done deactivating...\n", (int)@mandate);
	}

	// Creates a new lock record and adds it to the lock list.
	@.func add_new_lock (addr_t start, addr_t end) => (ref_t lock) {
		new_lock_rec () => (lock);
		lock->start = start;
		lock->end = end;
		add_lock_rec (lock);
	}

	// Allocates a new lock 'COPY' from the storage and copies all fields from LOCK to COPY.
	// The new record does _not_ get an own condition variable!
	@.define copy_lock_rec (LOCK) => (COPY) {
		new_lock_rec () => (COPY);
		memcpy(COPY, LOCK, sizeof *LOCK);
	}
	
	// Returns the first (=lowest start address) overlapping lock (NULL=>none found).
	@.define find_first_lock (start, end) => (result) {
		find_next_lock_from (@:>.lock_starting_first, start, end) => (result);
	}
	
	// Returns the overlapping lock with the next-higher start address from 'lock' (NULL=>no more found).
	@.func find_next_lock_from (ref_t lock, addr_t start, addr_t end) => (ref_t result) {
		for (result = lock; result != NULL_REF; result = next_start_from (result)) {
			if (result->end > start) {
				if (result->start >= end) {
					result = NULL_REF;
				}
				break;
			}
		}
		if (result != NULL_REF) {
			trace ("mand=%d Found lock [0x%llX..0x%llX] that overlaps region [0x%llX..0x%llX].\n", (int)@mandate, result->start, result->end, start, end);
		}
		else {
			trace ("mand=%d Found no lock that overlaps region [0x%llX..0x%llX].\n", (int)@mandate, start, end);
		}
	}
	
	// Prints out the lock list. Debug.
	@.func print_lock_list () {
		char buf[1024];
		int n;
		char *buf_end;
		ref_t lock, last_lock;
		lock = @:>.lock_starting_first;
		last_lock = NULL_REF;
		trace ("mand=%d lock list:\n", (int)@mandate);
		memset(buf, 0, sizeof buf);
		n = sizeof buf - 2;
		buf_end = buf;
		while (lock != NULL_REF && n > 0) {
			int len;
			snprintf(buf_end, n, "[0x%llX..0x%llX] ", lock->start, lock->end);
			len = strlen(buf_end);
			n -= len;
			buf_end += len;
			if (prev_start_from (lock) != last_lock) {
				trace ("mand=%d %s <-- INCONSISTENCY!!!\n", (int)@mandate, buf);
				return;
			}
			last_lock = lock;
			lock = next_start_from (lock);
		}
		if (@:>.lock_starting_last != last_lock) {
			trace ("mand=%d %s <-- FINAL INCONSISTENCY!!!\n", (int)@mandate, buf);
		}
		trace ("mand=%d %s (ok)\n", (int)@mandate, buf);
	}
	
	// Initializes/destroys status variables.
	// Stores status in brick variables. TODO: use a nest instead!
	operation $output_init {
		if (!@constr && @destr) {
			pthread_mutex_destroy(&@:>.lock_list_access);
			@success = TRUE;
		}
		if (@constr && !@destr) {
			int i;
			@.check(pthread_mutex_init(&@:>.lock_list_access, NULL) != 0, "Could not initialize lock_list_access mutex!");
			
			memset(@:>.lock_rec_storage, sizeof @:>.lock_rec_storage, 0);
			for (i = 0; i < LOCK_STORAGE_SIZE - 1; i++) {
				@:>.lock_rec_storage[i].next_start = &@:>.lock_rec_storage[i + 1];
			}
			@:>.lock_rec_storage[i].next_start = NULL_REF;
			@:>.lock_starting_first = NULL_REF;
			@:>.lock_starting_last = NULL_REF;
			@:>.free_lock_recs = @:>.lock_rec_storage;
			@:>.lock_trash = NULL_REF;
			@success = TRUE;
		}
	}

	// See description in brick header.
	operation $lock {
		addr_t obl_start = @log_addr;
		addr_t obl_end = @log_addr + @log_len;
		addr_t opt_start = @try_addr;
		addr_t opt_end = @try_addr + @try_len;
		ref_t conflictor;
		@.check (opt_start > obl_start || opt_end < obl_end, "Optional part [0x%llX..0x%llX] does not contain obligatory part [0x%llX..0x%llX]!", opt_start, opt_end, obl_start, obl_end);
		
		if (opt_end - opt_start == 0) {
			// Nothing to lock, return.
			trace ("mand=%d Requested lock [?0x%llX..[!0x%llX..0x%llX!]..0x%llX?] has no effect.\n", (int)@mandate, opt_start, obl_start, obl_end, opt_end);
			@success = TRUE;
			return;
		}
		
		pthread_mutex_lock(&@:>.lock_list_access);		
		trace ("mand=%d Lock [?0x%llX..[!0x%llX..0x%llX!]..0x%llX?] requested.\n", (int)@mandate, opt_start, obl_start, obl_end, opt_end);
		
		// Look for locks overlapping with the obligatory part of our requested lock.
		find_first_lock (obl_start, obl_end) => (conflictor);	
		if (obl_end - obl_start > 0) {
			if (conflictor != NULL_REF && @action != action_wait) {
				pthread_unlock_mutex(&@:>lock_list_access);
				return;
			}
			while (conflictor != NULL_REF) {
				// Wait for the lock to be released.
				pthread_cond_wait(&conflictor->unlocked, &@:>.lock_list_access);
				trace ("mand=%d got condition signal!\n", (int)@mandate);
				// Start all over again, there could be more locks in the way/ new locks may have appeared.
				find_first_lock (obl_start, obl_end) => (conflictor);
			}
		}
		else {
			// This is the very special case of a zero-length obligatory part truly inside a conflicting lock,
			// What results in a successful lock of zero bytes.
			// (Code below relies on no locks across the obligatory part)
			@try_addr = @log_addr;
			@try_len = 0;
			@success = TRUE;
			pthread_mutex_unlock(&@:>.lock_list_access);
			return;
		}
		
		// Cut the optional parts until nothing conflicts with them anymore.
		if (opt_start < obl_start || opt_end > obl_end) {
			find_first_lock (opt_start, opt_end) => (conflictor);
			while (conflictor != NULL_REF) {
				if (conflictor->end <= obl_start) {
					// Conflicting lock is below the obligatory part, cut optional part upwards.
					opt_start = conflictor->end;
				}
				else {
					// Conflicting is above the obligatory part, cut optional part downwards.
					opt_end = conflictor->start;
				}
				find_next_lock_from (conflictor, opt_start, opt_end) => (conflictor);
			}
		}
		
		// Now the way is free: insert a new lock record, if appropriate.
		if (opt_end > opt_start && @action != action_ask) {
			ref_t lock;
			add_new_lock (opt_start, opt_end) => (lock);
		}

		trace ("mand=%d Lock [?0x%llX [!0x%llX..0x%llX!] 0x%llX?] granted.\n", (int)@mandate, opt_start, obl_start, opt_end, obl_end);
		print_lock_list ();
		pthread_mutex_unlock(&@:>.lock_list_access);
		
		@try_addr = opt_start;
		@try_len = opt_end - opt_start;
		@success = TRUE;
	}
	
	// See description in brick header.
	operation $unlock {
		addr_t start;
		addr_t end;
		ref_t lock;
		// Unlocking will always succeed, so we unlock the whole optional part right away.
		start = @try_addr;
		end = @try_addr + @try_len;
		@.check (start > @log_addr || end < @log_addr + @log_len, "Optional part [0x%llX..0x%llX] does not contain obligatory part [0x%llX..0x%llX]!", start, end, @log_addr, @log_addr + @log_len);

		pthread_mutex_lock(&@:>.lock_list_access);
		trace ("mand=%d Unlock of [0x%llX..0x%llX] requested.\n", (int)@mandate, start, end);
		
		find_first_lock (start, end) => (lock);
		while (lock != NULL_REF) {
			ref_t next_lock = next_start_from (lock);
			// If the block is not completely covered by this $unlock, it must be split.
			// Create the (potential) lower and the upper leftovers as separate new locks.
			if (lock->start < start) {
				ref_t lower_part;
				copy_lock_rec (lock) => (lower_part);
				lower_part->end = start;
				add_lock_rec (lower_part);
			}
			if (lock->end > end) {
				ref_t upper_part;
				copy_lock_rec (lock) => (upper_part);
				upper_part->start = end;
				add_lock_rec (upper_part);
			}
			// Now remove the original lock. 
			remove_lock_rec (lock);
			free_lock_rec (lock);
			find_next_lock_from (next_lock, start, end) => (lock);
		}
		trace ("mand=%d Lock [0x%llX..0x%llX] released.\n", (int)@mandate, start, end);
		print_lock_list ();
		pthread_mutex_unlock(&@:>.lock_list_access);
		@success = TRUE;
	}

	// Forward any other operations to :<in.
	operation $op {
		@=outputcall :<in$op @args;
	}
