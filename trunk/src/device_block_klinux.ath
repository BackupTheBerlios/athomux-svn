  Author: Jens-Christian Korth
  Copyright: Jens-Christian Korth
  License: see files SOFTWARE-LICENSE, PATENT-LICENSE

context pconf: klinux

brick #device_block_klinux
purpose Wrapper for generic linux 2.6 block devices
desc
  This brick implements a kernelspace wrapper for block devices.
  It uses the new Block I/O (bio) interface of the 2.6 kernel.

  Restrictions:
  * The current implementation allows only DBK_RINGBUFFER_SIZE
    parallel transfer requests.
  * You are only allowed to call wait for the last
    DBK_RINGBUFFER_SIZE operations.
( * If you call wait for a request which failed,  )
(   the brick instance goes to an error state and )
(   doesnt allow any further request submissions. )
  * In the current implementation the instance enters
    into the error state already when the error occurs.

  NOTE: Linux drivers might require aligned requests,
        e.g. the floppy driver uses a 512 byte alignment.
enddesc
//attr transfer_size=0

static_header {
#include <linux/bio.h>
#include <linux/blkdev.h>
#include <linux/wait.h>
#include <asm/page.h>

#define DBK_RINGBUFFER_SIZE 3

#define BIO_EVALERR   7
#define BIO_INUSE     8
#define BIO_COMPLETED 9

struct dbk_request {
  struct bio        bio;
  struct bio_vec    bvec;
//struct page       page;
  addr_t            addr;
  prio_t            prio;
  wait_queue_head_t q;
  struct output     *on;
};
}

static_data {
#include <asm/bitops.h>
#include <asm/div64.h>

static void device_block_klinux_biodestructor(struct bio *bio)
{
  //printf("bio_destructor\n");
  clear_bit(BIO_INUSE, &bio->bi_flags);
  smp_mb__after_clear_bit();

  wait_queue_head_t *q = &BASE(((struct dbk_request *)bio->bi_private)->on, struct brick_device_block_klinux, _conn_out)->reqalloc_q;
  wake_up(q);
}


static int device_block_klinux_endio(struct bio *bio,
				     unsigned int bytes_done, int err)
{
  struct dbk_request *current_req = bio->bi_private;
  struct output *on = current_req->on;

  if (bio->bi_size)
    return 1;

  if(current_req->bvec.bv_len > bytes_done) {
    printf("device_block_klinux_endio: incomplete %s request (%i of %i bytes, flags=%lx), ",
	   (bio->bi_rw & WRITE) ? "WRITE" : "READ",
	   bytes_done, current_req->bvec.bv_len, bio->bi_flags);
    const unsigned int bytes_done_blockwise = bytes_done - (bytes_done % bdev_hardsect_size(bio->bi_bdev));
    if(bytes_done_blockwise > 0) {
      printf("requesting missing %i bytes again\n", bytes_done_blockwise);
      bio->bi_flags = ((bio->bi_flags & (1 << BIO_EVALERR)) |
		       ((1 << BIO_INUSE) | (1 << BIO_UPTODATE)));
      bio->bi_sector += bytes_done_blockwise / bdev_hardsect_size(bio->bi_bdev);
      current_req->bvec.bv_offset += bytes_done_blockwise;
      current_req->bvec.bv_len -= bytes_done_blockwise;
      bio->bi_size = current_req->bvec.bv_len;
      generic_make_request(bio); //submit_bio(bio->bi_rw, bio);
      return 0;
    } else {
      printf("failing\n");
      clear_bit(BIO_UPTODATE, &bio->bi_flags);
    }
  }

  set_bit(BIO_COMPLETED, &bio->bi_flags);
  trace("%s request %s\n",
	 (bio->bi_rw & WRITE) ? "WRITE" : "READ",
	 (bio_flagged(bio, BIO_UPTODATE)) ? "succeeded" : "failed");
//if(!bio_flagged(bio, BIO_UPTODATE) && !bio_flagged(bio, BIO_EVALERR)) {
  if(!(bio->bi_flags & ((1 << BIO_UPTODATE)|(1 << BIO_EVALERR)))) {
    // @#harderror_detected = TRUE;
    BASE(on, struct brick_device_block_klinux, _conn_out)->harderror_detected = TRUE;
  }

  wait_queue_head_t *q = &current_req->q;
  wake_up_all(q);
  
  bio_put(bio);
  return 0;
}


static struct dbk_request *dbkreq_alloc (const union connector *on)

{
  struct brick_device_block_klinux * const brick = BASE(on, struct brick_device_block_klinux, _conn_out);
  int start = brick->req_pos;
  int i = start;
  struct dbk_request *newreq = NULL;

  do {
    if (++i >= DBK_RINGBUFFER_SIZE) i = 0;
    newreq = &brick->req[i];
    if (!test_and_set_bit(BIO_INUSE, &newreq->bio.bi_flags)) {
      newreq->bio.bi_flags = (1 << BIO_INUSE) | (1 << BIO_UPTODATE);
      atomic_set(&newreq->bio.bi_cnt, 1);
      brick->req_pos = i;
      return newreq;
    }
  } while (i != start);
  return NULL;
}


static len_t get_maxlen(struct block_device *bdev)
{
  const request_queue_t *q = bdev_get_queue(bdev);

  return min(q->max_sectors << 9,
	     min(q->max_phys_segments, q->max_hw_segments) << PAGE_SHIFT);
}

}

data {
  struct block_device *bdev;
  struct dbk_request req[DBK_RINGBUFFER_SIZE];
  int req_pos;
  wait_queue_head_t reqalloc_q;
  bool harderror_detected;
  bool readonly;
}

output :>out


@.func dbkreq_find (int start) => (int index)
{
  for(index = start; index < DBK_RINGBUFFER_SIZE; index++) {
    if((@#req[index].prio >= @prio) &&
       ((@#req[index].addr <= @log_addr <= @#req[index].addr + @#req[index].bvec.bv_len) ||
	(@#req[index].addr <= @log_addr + @log_len <= @#req[index].addr + @#req[index].bvec.bv_len))) {
      break;
    }
  }
}

@.func dbkreq_dump (struct dbk_request *r, char *s)
{
#ifdef DEBUG
  printf("[%i %s]: prio=%x, logaddr=%llx, len=%x, size=%x, flags=%lx\n",
	 r-@#req, s,
	 r->prio, r->addr, r->bvec.bv_len, r->bio.bi_size,
	 r->bio.bi_flags);
#endif
}

@.func dbkreq_waiton (struct dbk_request *r)
{
  if (@action == action_wait) {
    wait_queue_head_t *q = &r->q;
    wait_event(*q, test_bit(BIO_COMPLETED, &r->bio.bi_flags));
    if(!bio_flagged(&r->bio, BIO_UPTODATE)) {
      // @#harderror_detected = TRUE;
      @success = FALSE;
    }
  }
}


operation $output_init
{
  int i;
  struct dbk_request *current_req;

  if(@destr) {
    if(!IS_ERR(@#bdev)) {
      close_bdev_excl(@#bdev);
    }
  }
  if(@constr) {
    @#req_pos = DBK_RINGBUFFER_SIZE-1;
    @#harderror_detected = FALSE;
    wait_queue_head_t *q = &@#reqalloc_q;
    init_waitqueue_head(q);
    if(IS_ERR(@#bdev = open_bdev_excl(@param, 0, (void *)on))) {
      @#bdev = open_bdev_excl(@param, MS_RDONLY, (void *)on);
      @#readonly = TRUE;
    } else {
      @#readonly = FALSE;
    }
    @.check(IS_ERR(@#bdev), "open_bdev_excl failed");

    for(i = 0; i < DBK_RINGBUFFER_SIZE; i++) {
      current_req = &@#req[i];

      bio_init(&current_req->bio);
      current_req->bio.bi_max_vecs = 1;
      current_req->bio.bi_io_vec = &current_req->bvec;
      current_req->bio.bi_destructor = device_block_klinux_biodestructor;

      current_req->bio.bi_bdev = @#bdev;
      current_req->bio.bi_vcnt = 1;
      current_req->bio.bi_end_io = device_block_klinux_endio;
      current_req->bio.bi_private = (void *)current_req;
      q = &current_req->q;
      init_waitqueue_head(q);
      current_req->on = (struct output *)on;
    }
  }
  @success = TRUE;
}

operation $transfer
{
  struct dbk_request *current_req;
  plen_t len = 0;
  
  if(IS_ERR(@#bdev) || (@#bdev == 0)) {
    @.err("$transfer called for invalid block device!");
  }

  switch(@direction) {
    case direct_write:
      if(@#readonly) return;
    case direct_read:
      if(@#harderror_detected) {
	@.err("error state - no new requests are allowed");
      }

      loff_t maxsize = @#bdev->bd_inode->i_size; // see ll_rw_blk.c:generic_make_request for linux 2.7 porting (bd_inode is deprecated)
      if (maxsize && (maxsize <= @log_addr)) {
	return;
      }

      // try to allocate a req; block if not available yet
      wait_queue_head_t *q = &@#reqalloc_q;
      wait_event(*q, (current_req=dbkreq_alloc(on)));
      if (current_req) {
#ifdef __i386__
	// work around missing 64bit division instruction
	uns8 sector = @log_addr;
	do_div(sector, bdev_hardsect_size(@#bdev));
	current_req->bio.bi_sector = sector;
#else
	// this is what we really want to to
	current_req->bio.bi_sector = @log_addr / bdev_hardsect_size(@#bdev);
#endif

	current_req->addr = @log_addr;
	current_req->prio = @prio;

	current_req->bvec.bv_page = virt_to_page(MAKE_PTR(@phys_addr));
	current_req->bvec.bv_offset = @phys_addr & (PAGE_SIZE-1);
#if 1
	// ignore page boundaries;
	// this is not intended by linux but will hopefully work
	len = min(@log_len, get_maxlen(@#bdev));
#else
	// restrict request to specified page
	len = min(@log_len, PAGE_SIZE - current_req->bvec.bv_offset);
#endif
	if(current_req->addr + len > maxsize) {
	  len = maxsize - current_req->addr;
	}
	current_req->bvec.bv_len = len;
	current_req->bio.bi_size = len;
	if(@op_code == opcode_tr) {
	  current_req->bio.bi_flags |= (1 << BIO_EVALERR);
	  // give request pointer to $tr
	  @phys_addr = MAKE_PADDR(current_req);
	}
	dbkreq_dump (current_req, "transfer");
	if(@direction == direct_write)
	  submit_bio(WRITE, &current_req->bio);
#if 0
	// READA will return an error instead of blocking when no request block is available
	else if(@prio == prio_background)
	  submit_bio(READA, &current_req->bio);
#endif
	else
	  submit_bio(READ, &current_req->bio);

#if 0
	// did submit_bio already complete the request (...errors?...)
	if(bio_flagged(&current_req->bio, BIO_COMPLETED))
	  printf("early completion\n");
#endif

	if(!bio_flagged(&current_req->bio, BIO_COMPLETED) ||
	   bio_flagged(&current_req->bio, BIO_UPTODATE)) {
	  @success = TRUE;
	  @phys_len = len;
	}
      }
      break;
    case direct_stop:
      @success = TRUE;
      @phys_len = 0;
      break;
      //    case direct_write:
    default:
      // @success = FALSE;
      break;
  }
}


operation $tr
{
  struct dbk_request *current_req;
  paddr_t oldpaddr = @phys_addr;
  @op_code = opcode_tr; // to be sure...
  device_block_klinux_out_0_transfer(on, @args, @param);
  //on->ops[@sect_code][opcode_transfer](on, @args, @param);

  current_req = MAKE_PTR(@phys_addr);
  @phys_addr = oldpaddr;

  if(!@success) {
    return;
  }

  @action = action_wait;
  dbkreq_dump (current_req, "wait(tr)");
  dbkreq_waiton (current_req);
}


operation $wait
{
  int index = -1;
  @success = TRUE;

  do {
    dbkreq_find (index+1) => (index);
    if (index >= DBK_RINGBUFFER_SIZE) {
      break;
    }

    dbkreq_dump (&@#req[index], "wait");
    dbkreq_waiton (&@#req[index]);
  } while(1);
}
