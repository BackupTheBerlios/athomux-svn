Author: Roland Niese
Copyright: Roland Niese
License: see files SOFTWARE-LICENSE, PATENT-LICENSE

context pconf: *
context cconf: *

brick #lock_cache

purpose outpost of #lock_mand

desc
       +-------------+
tmp---<:             |
       |             |
in----<:- - - - - - -:>---out
       +-------------+

A cache for lock resources to improve performance on long and/or slow paths between a lock manager and lock users.
The 'main' path where all operations except $lock, $unlock and $retract are simply forwarded on, is from :<in to :>out.
:<tmp is used to store information about the lock resources currently being held by this cache ('state'). Must be exclusive, and support non-overlapping $create and $delete, as well as $get, $put, $trans and $wait.
Requests for locks ($lock) not held by the cache get forwarded to the 'superordinate' lock manager somewhere behind :<in. Speculative locking can be added here through a brick that returns more locks than requested by the cache.
Requests for locks ($lock) held by the cache are immediately server from the cache's pool, without (possibly costly) lock requests to the lock manager.
Releases of locks ($unlock) only return locks to this cache's lock pool. It never returns any resources to the lock manager behind :<in without being asked to (via $retract).
This brick is totally stateless, it does not even use instance attributes. All runtime variables are stored externally in :<tmp.
This brick is, naturally, multiuser-safe.
enddesc

static_data {
	#include "param_scanner.h"

	@.define MAX_LOCK_RECORDS (1024)
	@.define STATE_STORAGE_SIZE (sizeof (struct brick_state))

	// Abstract reference to a lock record.
	typedef addr_t lock_ref_t;
	// Essential properties of a lock (without anything regarding dynamic data storage of the record).
	// start..start+length define the area locked
	// mandate defines the owner of the lock.
	// flags define the type of the lock (bits 0..1: data lock, bits 2..3: address lock, bit 4: 1=optional, currently unused)
	// Important: locks with this brick's own mandate are owned by the superordinate lock manager behind :<in.
	// Lock regions owned by this brick are specified by lack of any lock record.
	struct lock_desc {
		addr_t start;
		addr_t end;
		mand_t mandate;
		unsigned int flags;
	};
	// Lock record in a doubly linked list.
	// Also contains implementation of thread blocking.
	struct lock_rec {
		lock_ref_t next_start;
		lock_ref_t prev_start;
		addr_t native_lock;
		struct lock_desc desc;
	};
	// Structure of :<tmp, from address 0.
	// Lock records are stored in a doubly linked list, sorted by increasing start address.
	// This list is protected by a native mutex to guarantee consistence. 
	// This mutex is always reserved only for very short time (to read and update the list).
	// Particularly, the mutex is always released when the active operation must wait for another lock or before it 'leaves' this brick (via return or a synchronous operation call).
	struct brick_state {
		addr_t lock_starting_first;
		addr_t lock_starting_last;
		addr_t free_lock_recs;
		struct lock_rec lock_rec_storage[MAX_LOCK_RECORDS];		
	};
	// conflict matrix, packed bit arrays
	// conflict(A,B) <==> (lock_conflict_def[A.lock_type] & (1 << B.lock_type)) != 0
	// lock_type = ((addr_lock << 2) + data_lock)
	static unsigned short lock_conflict_def[] = {
		0x000, 0x444, 0x666, 0x000,
		0x700, 0x744, 0x766, 0x000,
		0x770, 0x774, 0x776
	};
	// lock types as characters, for debug messages only (0=none, 1=read, 2=write)
	static const char ch_lock_types[] = { '-', 'r', 'w' };

	// IN THE FOLLOWING MACROS, AN ARGUMENT NAMED 'REF' MEANS A 'struct lock_rec', A 'DESC' A 'struct lock_desc' INSTEAD

	// 'Primes' a lock: initializes the native lock mechanism.
	@.define activate_lock (REF) {
		success_t ok;
		@=outputcall ##lock_imp:>out(:0:)$gadr (1) => (ok, MAKE_STATE_PTR (REF)->native_lock);
		@.check (!ok, "Failed to obtain native lock!");
	}
	
	// Releases all threads waiting for this lock, then deactivates the native lock mechanism.
	@.define deactivate_lock (REF) {
		success_t ok;
		@=outputcall ##lock_imp:>out(:0:)$padr (MAKE_STATE_PTR (REF)->native_lock, 1) => (ok);
		@.check (!ok, "Failed to drop native lock!");
	}

	// Makes a thread wait for removal of this lock.
	@.define wait_for_lock (REF) {
		success_t ok;
		@=outputcall ##lock_imp:>out(:0:)$lock (MAKE_STATE_PTR (REF)->native_lock, 1) => (ok);
		@.check (!ok, "Failed to wait on native lock!");
	}
	
	// Reserve exclusive access to the lock list.
	@.define lock_mutex () {
		success_t ok;
		@=outputcall ##lock_imp:>out(:0:)$lock ((addr_t)-1, 1) => (ok);
		@.check (!ok, "Failed to reserve native mutex!");
	}
	
	// Release exclusive access to the lock list.
	@.define unlock_mutex () {
		success_t ok;
		@=outputcall ##lock_imp:>out(:0:)$unlock ((addr_t)-1, 1) => (ok);
		@.check (!ok, "Failed to release native mutex!");
	}

	// Abstract reference values/querys/operations.
	@.define NULL_REF ((lock_ref_t)0)

	// A valid physical pointer "state" to the brick state must be declared for these macros to work.
	@.define MAKE_STATE_PTR (REF) ((struct lock_rec *)((void *)((paddr_t)(state) + (plen_t)(REF))))
	@.define MAKE_STATE_REF (PTR) ((lock_ref_t)((char *)(PTR) - (char *)state))
	
	// Abstract operations on the lock list.
	@.define has_next_lock (REF) (MAKE_STATE_PTR (REF)->next_start != 0)
	@.define has_prev_lock (REF) (MAKE_STATE_PTR (REF)->prev_end != 0)
	@.define get_next_lock (REF) (MAKE_STATE_PTR (REF)->next_start)
	@.define get_prev_lock (REF) (MAKE_STATE_PTR (REF)->prev_start)
	@.define set_next_lock (REF, NEXT_REF) (MAKE_STATE_PTR (REF)->next_start = (NEXT_REF))
	@.define set_prev_lock (REF, PREV_REF) (MAKE_STATE_PTR (REF)->prev_start = (PREV_REF))
	
	// Getters and setters for lock descriptors (without lock list structure variables).
	@.define get_desc_start (DESC) ((DESC).start)
	@.define get_desc_end (DESC) ((DESC).end)
	@.define get_desc_mand (DESC) ((DESC).mandate)
	@.define get_desc_type (DESC) ((DESC).flags & 0x000F)
	@.define get_desc_data_type (DESC) ((DESC).flags & 0x0003)
	@.define get_desc_addr_type (DESC) (((DESC).flags & 0x000C) >> 2)
	@.define is_desc_obligatory (DESC) (((DESC).flags & 0x0010) >> 4)
	@.define set_desc_start (DESC, ADDR) ((DESC).start = (ADDR))
	@.define set_desc_end (DESC, ADDR) ((DESC).end = (ADDR))
	@.define set_desc_mand (DESC, MAND) ((DESC).mandate = (MAND))
	@.define set_desc_obligatory (DESC, IS_OBLIG) ((DESC).flags = (((DESC).flags & ~0x0010) | (IS_OBLIG << 4)))
	@.define set_desc_bounds (DESC, START_ADDR, END_ADDR) (set_desc_start (DESC, START_ADDR), set_desc_end (DESC, END_ADDR))
	@.define set_desc (DESC, ADDR_START, ADDR_END, MAND, TYPE) ((DESC).start = ADDR_START, (DESC).end = ADDR_END, (DESC).mandate = MAND, (DESC).flags = TYPE)
		
	// Getters and setters for lock records for the lock list.
	@.define get_lock_start (REF) (get_desc_start (MAKE_STATE_PTR (REF)->desc))
	@.define get_lock_end (REF) (get_desc_end (MAKE_STATE_PTR (REF)->desc))
	@.define get_lock_mand (REF) (get_desc_mand (MAKE_STATE_PTR (REF)->desc))
	@.define get_lock_type (REF) (get_desc_type (MAKE_STATE_PTR (REF)->desc))
	@.define get_lock_data_type (REF) (get_desc_data_type (MAKE_STATE_PTR (REF)->desc))
	@.define get_lock_addr_type (REF) (get_desc_addr_type (MAKE_STATE_PTR (REF)->desc))
	@.define is_lock_obligatory (REF) (is_desc_obligatory (MAKE_STATE_PTR (REF)->desc))
	@.define get_lock_desc (REF) (MAKE_STATE_PTR (REF)->desc)
	@.define set_lock_start (REF, ADDR) (set_desc_start (MAKE_STATE_PTR (REF)->desc, ADDR))
	@.define set_lock_end (REF, ADDR) (set_desc_end (MAKE_STATE_PTR (REF)->desc, ADDR))
	@.define set_lock_bounds (REF, START_ADDR, END_ADDR) (set_desc_bounds (MAKE_STATE_PTR (REF)->desc, START_ADDR, END_ADDR))
	@.define set_lock_mand (REF, MAND) (set_desc_mand (MAKE_STATE_PTR (REF)->desc, MAND))
	@.define set_lock_type (REF, LOCK_TYPE) (MAKE_STATE_PTR (REF)->desc.flags = ((MAKE_STATE_PTR (REF)->desc.flags & ~0x000F) | (LOCK_TYPE)))
	@.define set_lock_desc (REF, DESC) (MAKE_STATE_PTR (REF)->desc = (DESC))

	// Comparators and converters for lock types.
	@.define make_lock_type (DATA_LOCK, ADDR_LOCK, IS_OPT) (((IS_OPT) << 4) | ((ADDR_LOCK) << 2) | (DATA_LOCK))
	@.define is_weaker_or_equal (REF, DESC) (get_lock_data_type (REF) <= get_desc_data_type (DESC) && get_lock_addr_type (REF) <= get_desc_data_type (DESC) && is_lock_obligatory (REF) <= is_desc_obligatory (DESC))
	@.define is_equally_strong (REF, DESC) (get_lock_type (REF) == get_desc_type (DESC))
	@.define is_stronger_or_equal (REF, DESC) (get_lock_data_type (REF) >= get_desc_data_type (DESC) && get_lock_addr_type (REF) >= get_desc_addr_type (DESC) && is_lock_obligatory (REF) >= is_desc_obligatory (DESC))
	@.define set_strongest_lock_type (REF, DESC) (set_lock_type (REF, make_lock_type (get_lock_data_type (REF) > get_desc_data_type (DESC) ? get_lock_data_type (REF) : get_desc_data_type (DESC), get_lock_addr_type (REF) > get_desc_addr_type (DESC) ? get_lock_addr_type (REF) : get_desc_addr_type (DESC), is_lock_obligatory (REF) | is_desc_obligatory (DESC))))

	// Filter for lock record searches in the list of active locks.
	// Used by the functions find_first_lock and find_next_lock
	typedef bool (*lock_test_t) (struct brick_state *state, lock_ref_t lock, struct lock_desc *desc);
	
	// Tests a lock record in a list for conflict with a given lock descriptor.
	// Used to find conflicting locks in $lock.
	// No need to test for overlap, that is a precondition for the call of this filter.
	static bool is_conflict(struct brick_state *state, lock_ref_t lock, struct lock_desc *desc) {
		return (get_lock_mand (lock) != get_desc_mand (*desc) && lock_conflict_def[get_desc_type (*desc)] & (1 << get_lock_type (lock))) != 0;
	}

	// Tests a lock record in a list for 'ownage' by the same individual owning desc.
	// Used to find own locks in $lock (to melt them) and in $unlock (to split them).
	// No need to test for overlap, that is a precondition for the call of this filter.
	static bool is_own (struct brick_state *state, lock_ref_t lock, struct lock_desc *desc) {
		return get_lock_mand (lock) == get_desc_mand (*desc);
	}
}

@.macro DECLARE_THREAD_BRICK (NAME) {
	@.if ("PCONF" =~ m/^ulinux/) {
		instance #thread_ulinux as NAME
	}
	@.elsif ("PCONF" =~ m/^klinux/) {
		instance #thread_klinux as NAME
	}
}

// native implementation of locks
instance #lock_native as lock_imp;

// For asynchronous retracts. $lock calls to this brick will cause an asynchronous $retract
// on :>out. The operation $lock has only been chosen because it takes
// log_addr, log_len, try_addr, and try_len, just like $retract.
DECLARE_THREAD_BRICK (fork_a);
wire :<exec as #:>bounce;
wire :>cpu as #:<clients;

// For asynchronous requests of lock resources from the lock manager.
// Must be asynchronous to lessen the danger of deadlocks
DECLARE_THREAD_BRICK (fork_b);
wire :<exec as #:>loop;
wire :>cpu as #:<lpasync;

	// Allocates a new lock from the storage.
	@.define new_lock_rec (STATE_PTR) => (LOCK) {
		PC_DIRTY (tmp_state, 0);
		//trace ("mand=%d allocate new lock record...\n", (int)@mandate);
		LOCK = (STATE_PTR)->free_lock_recs;
		@.fatal (!LOCK, "No more free lock records!");
		//trace ("found free record.\n");
		(STATE_PTR)->free_lock_recs = get_next_lock (LOCK);
		//trace ("allocated free record.\n");
		@.if ("PCONF" =~ m/debug$/) {
			set_next_lock (LOCK, NULL_REF);
			set_prev_lock (LOCK, NULL_REF);
			set_lock_bounds(LOCK, 0, 0);
		}
	}
	
	// Returns a lock record to the list of free records.
	// The record must have been removed from the active list (via remove_lock_rec) before!
	@.define free_lock_rec (STATE_PTR, LOCK) {
		PC_DIRTY (tmp_state, 0);
		//trace ("mand=%d lock record freed [0x%llX..0x%llX]D%cA%c...\n", (int)@mandate, get_lock_start (LOCK), get_lock_end (LOCK), ch_lock_types[get_lock_data_type (LOCK)], ch_lock_types[get_lock_addr_type (LOCK)]);
		set_next_lock (LOCK, (STATE_PTR)->free_lock_recs);
		set_prev_lock (LOCK, NULL_REF);
		(STATE_PTR)->free_lock_recs = (LOCK);
	}

	// Activates a lock record and adds it to the list.
	@.func add_lock_rec (struct brick_state *state, lock_ref_t lock) {
		lock_ref_t scan;
		PC_DIRTY (tmp_state, 0);
		trace ("mand=%d added new lock record [0x%llX..0x%llX]D%cA%c...\n", (int)@mandate, get_lock_start (lock), get_lock_end (lock), ch_lock_types[get_lock_data_type (lock)], ch_lock_types[get_lock_addr_type (lock)]);
		for (scan = state->lock_starting_first; scan != NULL_REF && get_lock_start (scan) < get_lock_start (lock); scan = get_next_lock (scan));
		activate_lock (lock);
		set_next_lock (lock, scan);
		if (get_next_lock (lock) != NULL_REF) {
			lock_ref_t next_lock = get_next_lock (lock);
			lock_ref_t prev_lock = get_prev_lock (next_lock);
			set_prev_lock (lock, prev_lock);
			set_prev_lock (next_lock, lock);
		}
		else {
			set_prev_lock (lock, state->lock_starting_last);
			state->lock_starting_last = lock;
		}
		if (get_prev_lock (lock) != NULL_REF) {
			lock_ref_t prev_lock = get_prev_lock (lock);
			set_next_lock (prev_lock, lock);
		}
		else {
			state->lock_starting_first = lock;
		}
	}
	
	// Deactivates (signals and destroys condition) and removes a lock record from the list.
	@.func remove_lock_rec (struct brick_state *state, lock_ref_t lock) {
		PC_DIRTY (tmp_state, 0);
		trace ("mand=%d removed lock record [0x%llX..0x%llX]D%cA%c...\n", (int)@mandate, get_lock_start (lock), get_lock_end (lock), ch_lock_types[get_lock_data_type (lock)], ch_lock_types[get_lock_addr_type (lock)]);
		if (get_prev_lock (lock) != NULL_REF) {
			lock_ref_t prev_lock = get_prev_lock (lock);
			lock_ref_t next_lock = get_next_lock (lock);
			set_next_lock (prev_lock, next_lock);
		}
		else {
			state->lock_starting_first = get_next_lock (lock);
		}
		if (get_next_lock (lock) != NULL_REF) {
			lock_ref_t next_lock = get_next_lock (lock);
			lock_ref_t prev_lock = get_prev_lock (lock);
			set_prev_lock (next_lock, prev_lock);
		}
		else {
			state->lock_starting_last = get_prev_lock (lock);
		}
		deactivate_lock (lock);
	}

	// Allocates a new lock 'COPY' from the storage and copies
	// LOCK's essential characteristics to COPY. List and condition variables
	// are not copied.
	@.define copy_lock_rec (STATE, LOCK) => (COPY) {
		new_lock_rec (STATE) => (COPY);
		@.check ((COPY) == NULL_REF, "Out of lock records!");
		set_lock_desc (COPY, get_lock_desc (LOCK));
	}
	
	// Returns the first (=lowest start address) lock record in the list that
	// overlaps the specified descriptor and fulfills the specified requirement.
	@.define find_first_lock (STATE_PTR, DESC_PTR, REQUIREMENT) => (RESULT) {
		find_next_lock_from (STATE_PTR, (STATE_PTR)->lock_starting_first, DESC_PTR, REQUIREMENT) => (RESULT);
	}
	
	// Returns the next (=next-higher start address from 'lock') lock record in the list that
	// overlaps the specified descriptor and fulfills the specified requirement.
	@.func find_next_lock_from (struct brick_state *state, lock_ref_t lock, struct lock_desc *desc, lock_test_t requirement) => (lock_ref_t result) {
		for (result = lock; result != NULL_REF; result = get_next_lock (result)) {
			if (get_lock_end (result) > get_desc_start (*desc)) {
				if (get_lock_start (result) >= get_desc_end (*desc)) {
					result = NULL_REF;
					break;
				}
				if (requirement (state, result, desc)) {
					break;
				}
			}
		}
		if (result != NULL_REF) {
			trace ("mand=%d Found lock (%d)[0x%llX..0x%llX]D%cA%c in region [0x%llX..0x%llX].\n", (int)@mandate, get_lock_mand (result), get_lock_start (result), get_lock_end (result), ch_lock_types[get_lock_data_type (lock)], ch_lock_types[get_lock_addr_type (lock)], get_desc_start (*desc), get_desc_end (*desc));
		}
		else {
			trace ("mand=%d Found no lock in region [0x%llX..0x%llX].\n", (int)@mandate, get_desc_start (*desc), get_desc_end (*desc));
		}
	}
	
	// Melts the lock described into the existing lock list.
	// This function ignores conflicting locks. The lock gets molten with existing own locks.
	// Write locks override read locks.
	// Existing locks are kept where possible, simply 'upgrading' them, to avoid unnecessary wake-ups.
	// At desc's borders however, existing locks possibly have to be split (replaced by two locks of different grade).
	// Gaps are filled with new lock records with desc's grade.
	@.func melt_lock_into_list (struct brick_state *state, struct lock_desc desc) => (success_t ok) {
		lock_ref_t lock;
		ok = FALSE;
		trace ("Melting lock (%d)[0x%llX..0x%llX]D%cA%c into lock list\n", get_desc_mand (desc), get_desc_start (desc), get_desc_end (desc), ch_lock_types[get_desc_data_type (desc)], ch_lock_types[get_desc_addr_type (desc)]);
		find_first_lock (state, &desc, is_own) => (lock);
		if (lock != NULL_REF) {
			// There is at least one block with the same mandate overlapping the new lock.
			lock_ref_t prev_lock = NULL_REF;
			if (get_lock_start (lock) < get_desc_start (desc)) {
				// The lock only partly overlaps the new lock. It may be necessary to split it.
				if (!is_stronger_or_equal (lock, desc)) {
					trace ("mand=%d Split partly overlapping lock:\n", (int)@mandate);
					// The lock is (partly) weaker than the new lock and 
					// only the overlapping part has to be enstrengthened.
					lock_ref_t new_lock;
					new_lock_rec (state) => (new_lock);
					@.check (new_lock == NULL_REF, "Out of lock records!");
					remove_lock_rec (state, lock);
					set_lock_desc (new_lock, get_lock_desc (lock));
					set_lock_end (new_lock, get_desc_start (desc));
					set_lock_start (lock, get_desc_start (desc));
					// Do not enstrengthen the lock yet, it could be necessary to split it further. Let the loop do that
					add_lock_rec (state, lock);
					add_lock_rec (state, new_lock);
				}
				// 'lock' needs to be investigated further
			}
			while (lock != NULL_REF) {
				// Loop invariant: there is no own lock between start of 'desc' and start of 'lock'.
				if (get_lock_start (lock) > get_desc_start (desc)) {
					trace ("mand=%d there is a gap to fill from 0x%llX to 0x%llX\n", get_desc_mand (desc), get_desc_start (desc), get_lock_start (lock));
					// There is a gap to fill to the next existing lock.
					if (is_weaker_or_equal (lock, desc)) {
						trace ("mand=%d filling gap by stretching next lock.\n", (int)@mandate);
						set_lock_start (lock, get_desc_start (desc));
					}
					else if (prev_lock != NULL_REF && is_equally_strong (prev_lock, desc)) {
						trace ("mand=%d filling gap by stretching previous lock.\n", (int)@mandate);
						set_lock_end (prev_lock, get_lock_start (lock));
					}
					else {
						lock_ref_t new_lock;
						trace ("mand=%d filling gap with a new record:\n", (int)@mandate);
						new_lock_rec (state) => (new_lock);
						@.check (new_lock == NULL_REF, "Out of lock records!");
						set_lock_desc (new_lock, desc);
						set_lock_end (new_lock, get_lock_start (lock));
						add_lock_rec (state, new_lock);
					}
				}
				if (get_lock_end (lock) > get_desc_end (desc)) {
					trace ("mand=%d The final lock exceeds the region to be locked.\n", (int)@mandate);
					// The existing lock exceeds the new lock and might need to be split.
					if (!is_stronger_or_equal (lock, desc)) {
						// It is (partly) weaker, and the overlapping part needs
						// enstrengthment, so split it.
						lock_ref_t new_lock;
						trace ("mand=%d splitting the lock\n", @mandate);
						new_lock_rec (state) => (new_lock);
						@.check (new_lock == NULL_REF, "Out of lock records!");
						remove_lock_rec (state, lock);
						set_lock_desc (new_lock, get_lock_desc (lock));
						set_lock_start (new_lock, get_desc_end (desc));
						set_lock_end (lock, get_desc_end (desc));
						add_lock_rec (state, lock);
						add_lock_rec (state, new_lock);
					}
				}
				// If the lock is (partly) weaker, enstrengthen it.
				set_strongest_lock_type (lock, desc);
				set_desc_start (desc, get_lock_end (lock));
				if (get_desc_start (desc) >= get_desc_end (desc)) {
					break;
				}
				prev_lock = lock;
				find_next_lock_from (state, lock, &desc, is_own) => (lock);
			}
		}
		if (get_desc_start (desc) < get_desc_end (desc)) {
			// Behind the last existing lock there is still some empty space left to lock.
			lock_ref_t lock;
			new_lock_rec (state) => (lock);
			@.check (lock == NULL_REF, "Out of lock records!");
			set_lock_desc (lock, desc);
			add_lock_rec (state, lock);
		}
		ok = TRUE;
	}

	@.if ("PCONF" =~ m/debug$/) {
		// Debug function: checks consistency of and prints out the lock list.
		@.func print_lock_list (struct brick_state *state) => (success_t ok) {
			char buf[1024];
			int n;
			char *buf_end;
			lock_ref_t lock, prev_lock;
			addr_t prev_start;
			ok = FALSE;
			trace ("mand=%d lock list:\n", (int)@mandate);
			memset(buf, 0, sizeof buf);
			n = sizeof buf - 2;
			buf_end = buf;
			// Print out the list, check pointer consistency and order of start addresses.
			prev_lock = NULL_REF;
			prev_start = 0;
			lock = state->lock_starting_first;
			while (lock != NULL_REF && n > 0) {
				int len;
				snprintf(buf_end, n, "(%d)[0x%llX..0x%llX]D%cA%c ", (int)get_lock_mand (lock), get_lock_start (lock), get_lock_end (lock), ch_lock_types[get_lock_data_type (lock)], ch_lock_types[get_lock_addr_type (lock)]);
				len = strlen(buf_end);
				n -= len;
				buf_end += len;
				if (get_prev_lock (lock) != prev_lock) {
					trace ("%s <-- INCONSISTENCY!!!\n", buf);
					return;
				}
				if (get_lock_start (lock) < prev_start) {
					trace ("%s <-- BROKEN ORDER!!!\n", buf);
				}
				prev_lock = lock;
				prev_start = get_lock_start (prev_lock);
				lock = get_next_lock (lock);
			}
			if (state->lock_starting_last != prev_lock) {
				trace ("%s <-- INCONSISTENCY AT END OF LIST!!!\n", buf);
			}
			trace ("mand=%d %s (pointers consistent)\n", (int)@mandate, buf);

			// Check for non-disjunct locks with same mandate.
			prev_lock = state->lock_starting_first;
			while (prev_lock != NULL_REF) {
				for (lock = get_next_lock (prev_lock); lock != NULL_REF && get_lock_start (lock) < get_lock_end (prev_lock); lock = get_next_lock (lock)) {
					if (get_lock_mand (lock) == get_lock_mand (prev_lock)) {
						trace ("LOCKS OF SAME MANDATE OVERLAP: (%d)[0x%llX..0x%llX]D%cA%c and (%d)[0x%llX..0x%llX]D%cA%c!\n", get_lock_mand (lock), get_lock_start (lock), get_lock_end (lock), ch_lock_types[get_lock_data_type (lock)], ch_lock_types[get_lock_addr_type (lock)], get_lock_mand (prev_lock), get_lock_start (prev_lock), get_lock_end (prev_lock), ch_lock_types[get_lock_data_type (prev_lock)], ch_lock_types[get_lock_addr_type (prev_lock)]);
						return;
					}
				}
				prev_lock = get_next_lock (prev_lock);
			}
			ok = TRUE;
		}
	}
	
	// Internal unlock function.
	// Removes all locks with the same mandate in the specified range.
	// Splits existing locks where necessary.
	@.func remove_own_locks (struct brick_state *state, struct lock_desc *desc) => (success_t ok) {
		lock_ref_t lock;
		trace ("mand=%d removing all own locks in [0x%llX..0x%llX]\n", get_desc_mand (*desc), get_desc_start (*desc), get_desc_end (*desc));
		find_first_lock (state, desc, is_own) => (lock);
		while (lock != NULL_REF) {
			lock_ref_t next_lock = get_next_lock (lock);
			// If the block only party overlaps the range to unlock, it must be split.
			// Create the (potential) lower and the upper leftovers as separate new locks.
			if (get_lock_start (lock) < get_desc_start (*desc)) {
				lock_ref_t lower_part;
				trace ("mand=%d must split the lock at 0x%llX\n", get_desc_mand (*desc), get_desc_start (*desc));
				copy_lock_rec (state, lock) => (lower_part);
				set_lock_end (lower_part, get_desc_start (*desc));
				add_lock_rec (state, lower_part);
			}
			if (get_lock_end (lock) > get_desc_end (*desc)) {
				trace ("mand=%d must split the lock at 0x%llX\n", get_desc_mand (*desc), get_desc_start (*desc));
				lock_ref_t upper_part;
				copy_lock_rec (state, lock) => (upper_part);
				set_lock_start (upper_part, get_desc_end (*desc));
				add_lock_rec (state, upper_part);
			}
			// Now remove the original lock. 
			remove_lock_rec (state, lock);
			free_lock_rec (state, lock);
			find_next_lock_from (state, next_lock, desc, is_own) => (lock);
		}
		ok = TRUE;
	}

operation $brick_init {
	if (@destr) {
		INIT_ALL_OUTPUTS ();
		INIT_ONE_INSTANCE (lock_imp, "");
		INIT_ONE_INSTANCE (fork_b, "");
		INIT_ONE_INSTANCE (fork_a, "");
		INIT_ALL_INPUTS ();
	}
	if (@constr) {
		INIT_ALL_INPUTS ();
		INIT_ONE_INSTANCE (fork_a, "keep_mandates");
		INIT_ONE_INSTANCE (fork_b, "keep_mandates");
		INIT_ONE_INSTANCE (lock_imp, "");
		INIT_ALL_OUTPUTS ();
	}
	//XXX: nasty but necessary, since INIT_ALL_OUTPUTS does not init internal connectors
	@=outputcall :>bounce$output_init (@destr, @constr) => ();
	@=inputcall :<clients$input_init (@destr, @constr) => ();
	@=outputcall :>loop$output_init (@destr, @constr) => ();
	@=inputcall :<lpasync$input_init (@destr, @constr) => ();
}

// Lock requests ($lock) at this input get asynchronously forwarded to :>out, as $retract.
local input :<clients
// these two are connected through a #thread brick
local output :>bounce

	operation $lock {
		trace ("asynchronous $retract (%d)[0x%llX..0x%llX]\n", @mandate, @try_addr, @try_addr + @try_len);
		@=inputcall :>out$retract [@mandate] @args;
	}

// 
local input :<lpasync
// these two are connected through a thread brick
local output :>loop

	// A $retract from the superordinate has been transformed into a $lock with this brick's mandate.
	// When this operation has succeeded, the lock held by this cache must of course be returned to the superordinate.
	operation $lock {
		success_t ok;
		@=outputcall :>out$lock [@#._mand] (@log_addr, @log_len, lock_write, lock_write, @log_addr, @log_len, action_wait) => (ok, @try_addr, @try_len);
		if (!ok) {
			// Danger of a deadlock. 
			// TODO: instead of aborting, we should wait for a change in the area tried to be retracted and then try again.
			// TODO: remember, we signaled "OK" at the $retract op. Now the lock should be actually returned some time, we risk an 'urgent' retraction.
			return;
		}
		else {
			// DISCUSS: We are in an inconsistent state now! This brick has now marked the lock as released, but has not returned it yet!
			// What happens on concurrent $lock ops in this area in the meantime? Should it be protected until successfully returned?
			// Not necessary if we can rely on consistent order of operation delivery over the wires.
			@=outputcall :<in$unlock [@#._mand] (@try_addr, @try_len) => (ok);
			// $unlock is always expected to succeed.
			@.fatal (!ok, ":<in$unlock (%d)[0x%llX..0x%llX] failed! Should not! Serious error!", @#._mand, @try_addr, @try_addr + @try_len);
		}
		// result is meaningless anyway, since this call was an asynchronous one.
		@success = TRUE;
	}

input :<in
	
	// Retracts from superordinate lock manager is treated like an asynchronous $lock op success of which is signalled by :<in$unlock.
	// See input :<lpasync for implementation
	operation $retract {
		trace ("retraction (%d)[0x%llX..0x%llX] received from superordinate lock manager\n", @mandate, @try_addr, @try_addr + @try_len);
		if (@mandate != @#._mand) {
			@=outputcall :<lpasync$lock @args;
		}
		else {
			trace ("The retraction is in my name! But I cannot conflict for resources with myself! This must result from a lock acquisition by me, ignore...\n");
		}
		@success = TRUE;
	}

	// Future, yet unknown, ops are redirected to :>out, since this brick does not handle them. 
	operation $op {
		@=inputcall :>out$op @args;
	}
	
input :<tmp

	use PC tmp_state [4] ;

output :>out(:2:)

	// Record type of the meta nest, describes one lock entry.
	// Field "flags": bits 0..1=data lock type, 2..3=address lock type
	define export TYPE lockrec_t "addr_t addr, len_t len, mand_t mand, int4 flags";

// section 0: main nest - lock management
// section 1: meta nest - surveillance
section (:0:)
	
	// Initializes/destroys status variables.
	operation $output_init {
		success_t ok;
		trace ("#lock_cache:>out$output_init (constr==%d, desctr==%d) : \"%s\" called\n", @constr, @destr, @param);
		if (@destr) {
			PC_FLUSH (tmp_state);
			@=outputcall :<tmp$delete (0, STATE_STORAGE_SIZE) => (ok);
			@.check (!ok, "Could not destroy state storage!");
		}
		if (@constr) {
			struct brick_state *state;
			int i;
			
			lock_ref_t lock;
			@=outputcall :<tmp$create (0, STATE_STORAGE_SIZE, TRUE, FALSE) => (ok);
			state = PC_GET_DIRTY (tmp_state, 0, STATE_STORAGE_SIZE);
			@.check (!ok || !state, "Could not get state storage!");
			for (i = 0; i < MAX_LOCK_RECORDS - 1; i++) {
				state->lock_rec_storage[i].next_start = MAKE_STATE_REF (&state->lock_rec_storage[i + 1]);
			}
			state->lock_rec_storage[i].next_start = NULL_REF;
			state->lock_starting_first = NULL_REF;
			state->lock_starting_last = NULL_REF;
			state->free_lock_recs = MAKE_STATE_REF (state->lock_rec_storage);
			new_lock_rec (state) => (lock);
			@.fatal (lock == NULL_REF, "Could not even allocate one lock record!");
			set_desc (get_lock_desc (lock), (addr_t)0, (addr_t)0xFFFFFFFFFFFFFFFFLL, @#._mand, make_lock_type (lock_write, lock_write, FALSE));
			add_lock_rec (state, lock);
			
			if (param_check(@param, "lock_all_on_init", NULL, NULL)) {
				// Special init parameter to speculatively lock whole :>in initially.
				// Handy for performance improvement, if this lock cache is the only client anyway (like in #lock_manager).
				struct lock_desc desc;
				trace ("allocating all locks from the superordinate lock manager.\n");
				@=outputcall :<in$lock (0, 0xFFFFFFFFFFFFFFFFLL, @#._mand, make_lock_type (lock_write, lock_write, FALSE)) => (ok);
				@.check (!ok, "Could not grab all locks initially!");
				lock_mutex ();
				state = PC_GET_DIRTY (tmp_state, 0, STATE_STORAGE_SIZE);
				@.check (!state, "Could not get state storage!");
				set_desc (desc, 0, 0xFFFFFFFFFFFFFFFFLL, @#._mand, make_lock_type (lock_write, lock_write, FALSE));
				remove_own_locks (state, &desc) => (ok);
				@.check (!ok, "BUG! Could not release all locks!");
				unlock_mutex ();
			}
		}
		@success = TRUE;
	}

	// See description in brick header.
	operation $lock {
		addr_t obl_start = @log_addr;
		addr_t obl_end = @log_addr + @log_len;
		addr_t opt_start = @try_addr;
		addr_t opt_end = @try_addr + @try_len;
		struct brick_state *state;
		struct lock_desc desc;
		lock_ref_t conflictor;
		@.check (opt_end < opt_start || obl_end < obl_start, "Lock  [0x%llX..[0x%llX..0x%llX]..0x%llX] exceeds address space!", opt_start, obl_start, obl_end, opt_end);
		@.check (opt_start > obl_start || opt_end < obl_end, "Optional part [0x%llX..0x%llX] does not contain obligatory part [0x%llX..0x%llX]!", opt_start, opt_end, obl_start, obl_end);
		
		if (opt_end - opt_start == 0 || (@data_lock == lock_none && @addr_lock == lock_none)) {
			// Nothing to lock, return.
			trace ("mand=%d Requested lock [?0x%llX..[!0x%llX..0x%llX!]..0x%llX?]D%cA%c would have no effect.\n", (int)@mandate, opt_start, obl_start, obl_end, opt_end, ch_lock_types[@data_lock], ch_lock_types[@addr_lock]);
			@success = TRUE;
			return;
		}
		
		set_desc (desc, obl_start, obl_end, @mandate, make_lock_type (@data_lock, @addr_lock, TRUE));

		lock_mutex ();
		trace ("Lock (%d)[?0x%llX..[!0x%llX..0x%llX!]..0x%llX?]D%cA%c requested.\n", (int)@mandate, opt_start, obl_start, obl_end, opt_end, ch_lock_types[@data_lock], ch_lock_types[@addr_lock]);
		
		state = PC_GET (tmp_state, 0, STATE_STORAGE_SIZE);
		@.check (!state, "Error getting brick status!");
		
		trace ("Looking for conflicts\n");
		// Look for locks conflicting with the obligatory part of our requested lock.
		find_first_lock (state, &desc, is_conflict) => (conflictor);
		if (obl_end - obl_start > 0) {
			if (conflictor != NULL_REF) {
				if (@action != action_wait) {
					// There is a conflict and we are not supposed to wait for it, so break.
					unlock_mutex ();
					// No metanest notification on failed operations.
					return;
				}
				do {
					success_t ok;
					addr_t start;
					len_t length;
					start = get_lock_start (conflictor);
					length = get_lock_end (conflictor) - start;
					if (start < get_desc_start (desc)) {
						// conflictor overlaps only partially (at beginning), we don't need to request all of it
						len_t diff = get_desc_start (desc) - start;
						start += diff;
						length -= diff;
					}
					if (start + length > get_desc_end (desc)) {
						// conflictor overlaps only partially (at end), we don't need to request all of it
						length -= (start + length) - get_desc_end (desc);
					}
					if (get_lock_mand (conflictor) != @#._mand) {
						// The conflictor is owned by another client at :>out.
						// All we can do is signal need for the lock by a retraction and then wait for the lock to be returned.
						//trace ("about to send out an asynchronous $retract [0x%llX..0x%llX]\n", start, start + length);						
						// A $lock call through ##clntasync becomes an asynchronous inputcall :>out$retract
						@=outputcall :<clients$lock [@mandate] (start, length) => (ok);
						//trace ("sent out an asynchronous $retract [0x%llX..0x%llX]\n", start, start + length);
						// Wait for the lock to be released.
						wait_for_lock (conflictor);
						trace ("back to lock request (%d)[?0x%llX..[!0x%llX..0x%llX!]..0x%llX?]D%cA%c...\n", (int)@mandate, opt_start, obl_start, obl_end, opt_end, ch_lock_types[@data_lock], ch_lock_types[@addr_lock]);
						// In case the state pointer has been removed from the cache in the meantime.
						state = PC_GET (tmp_state, 0, STATE_STORAGE_SIZE);
					}
					else {
						// The conflictor is owned by the superordinate lock manager (at :<in).
						// Instead of retraction, we must send a request for the lock via a $lock op.
						// We prepare for speculative locking: the return of more locks than requested to improve performance of future lock requests.
						// Release the mutex while we are "out" to avoid deadlocks.
						unlock_mutex ();
						addr_t spec_addr;
						len_t spec_len;
						@=outputcall :<in$lock [@#._mand] (start, length, lock_write, lock_write, start, length, action_wait) => (ok, spec_addr, spec_len);
						// Operation failure would signal either a connection failure or the danger of a deadlock.
						@.check (!ok, "Lock [0x%llX..0x%llX] could not be acquired from superordinate lock manager!", get_desc_start (desc), get_desc_end (desc));
						lock_mutex ();
						// The lock list could have changed completely since we left the brick.
						trace ("back to lock request (%d)[?0x%llX..[!0x%llX..0x%llX!]..0x%llX?]D%cA%c...\n", (int)@mandate, opt_start, obl_start, obl_end, opt_end, ch_lock_types[@data_lock], ch_lock_types[@addr_lock]);
						// In case the state pointer has been removed from the cache in the meantime.
						state = PC_GET (tmp_state, 0, STATE_STORAGE_SIZE);
						// The $lock has been granted <==> has been successfully 'retracted' from the superordinate lock manager.
						// Remove all locks just granted by 'daddy'.
						struct lock_desc grant_desc;
						set_desc (grant_desc, spec_addr, spec_addr + spec_len, @#._mand, make_lock_type (lock_write, lock_write, FALSE));
						remove_own_locks (state, &grant_desc) => (ok);
						@.check (!ok, "Internal lock record release failed!"); // (should never happen)
					}
					// Start all over again, because the locks have changed in the meantime
					// and the reference 'conflictor' might have become invalid.
					find_first_lock (state, &desc, is_conflict) => (conflictor);
				} while (conflictor != NULL_REF);
			}
		}
		else {
			if (conflictor != NULL_REF) {
				// This is the very special case of a zero-length obligatory part truly inside a conflicting lock,
				// What results in a successful lock of zero bytes.
				// (Code below relies on no locks spanning across the obligatory part)
				@try_addr = @log_addr;
				@try_len = 0;
				@success = TRUE;
				trace ("mand=%d Address 0x%llX truly contained inside lock [0x%llX..0x%llX], \"locked\" 0 bytes.\n", (int)@mandate, obl_start, get_lock_start (conflictor), get_lock_end (conflictor));
				unlock_mutex ();
				return;
			}
		}
		
		// Cut the optional parts until nothing conflicts with them anymore.
		if (opt_start < obl_start || opt_end > obl_end) {
			set_desc_bounds (desc, opt_start, opt_end);
			find_first_lock (state, &desc, is_conflict) => (conflictor);
			while (conflictor != NULL_REF) {
				if (get_lock_end (conflictor) <= obl_start) {
					// Conflicting lock is below the obligatory part, cut optional part upwards.
					set_desc_start (desc, get_lock_end (conflictor));
				}
				else {
					// Conflicting is above the obligatory part, cut optional part downwards.
					set_desc_end (desc, get_lock_start (conflictor));
				}
				find_next_lock_from (state, conflictor, &desc, is_conflict) => (conflictor);
			}
			trace ("mand=%d Optional lock [0x%llX..0x%llX] cut to [0x%llX..0x%llX].\n", (int)@mandate, opt_start, opt_end, get_desc_start (desc), get_desc_end (desc));
		}
		
		// Now the way is free to add the lock. Add new lock records only where there is none
		// yet owned by this mandate.
		@try_addr = get_desc_start (desc);
		@try_len = get_desc_end (desc) - get_desc_start (desc);
		if (get_desc_end (desc) > get_desc_start (desc) && @action != action_ask) {
			success_t ok;
			set_desc_obligatory (desc, TRUE);
			melt_lock_into_list (state, desc) => (ok);
			if (!ok) {
				// Do not notify the metanest on failure.
				return;
			}
		}
		trace ("mand=%d Lock [0x%llX..0x%llX]D%cA%c granted.\n", (int)@mandate, @try_addr, @try_addr + @try_len, ch_lock_types[@data_lock], ch_lock_types[@addr_lock]);
		@.if ("PCONF" =~ m/debug$/) {
			print_lock_list (state) => (@success);
		}
		@.else {
			@success = TRUE;
		}
		unlock_mutex ();
	}
	
	// See description in brick header.
	operation $unlock {
		struct lock_desc desc;
		struct brick_state *state;
		success_t ok;
		// Unlocking will always succeed, so we unlock the whole optional part right away.
		set_desc (desc, @try_addr, @try_addr + @try_len, @mandate, make_lock_type (lock_write, lock_write, TRUE));
		@.check (get_desc_start (desc) > @log_addr || get_desc_end (desc) < @log_addr + @log_len, "Optional part [0x%llX..0x%llX] does not contain obligatory part [0x%llX..0x%llX]!", get_desc_start (desc), get_desc_end (desc), @log_addr, @log_addr + @log_len);
		lock_mutex ();
		trace ("mand=%d Unlock of [0x%llX..0x%llX] requested.\n", (int)@mandate, get_desc_start (desc), get_desc_end (desc));
		
		state = PC_GET (tmp_state, 0, STATE_STORAGE_SIZE);
		@.check (!state, "Error getting brick status!");
		
		remove_own_locks (state, &desc) => (ok);
		if (!ok) {
			unlock_mutex ();
			@.abort ("Serious error when removing locks in [0x%llX..0x%llX]\n", get_desc_start (desc), get_desc_end (desc));
		}
		
		trace ("mand=%d Lock [0x%llX..0x%llX] released.\n", (int)@mandate, get_desc_start (desc), get_desc_end (desc));
		@.if ("PCONF" =~ m/debug$/) {
			print_lock_list (state) => (@success);
		}
		@.else {
			@success = TRUE;
		}
		unlock_mutex ();
	}

	// Forward any other operations to :<in.
	operation $op {
		@=outputcall :<in$op @args;
	}

// section 1: meta nest
// for external surveillance
// read from address 0 via $transwait, buffer gets filled with an array of "lockrec_t" records (exported type)
section (:1:)

	// Writes the current main nest structure to the meta nest buffer
	// The main nest mutex must be owned by whoever this function gets called by
	@.func write_metainfo (void *buf, plen_t buf_size) => (success_t ok, plen_t buf_len) {
		struct brick_state *state;
		lock_ref_t lock_ref;
		void *buf_writeptr;
		plen_t buf_available = buf_size;
		
		state = PC_GET (tmp_state, 0, STATE_STORAGE_SIZE);
		ok = FALSE;
		@.check (!state, "Error getting brick status!");
		
		lock_ref = state->lock_starting_first;
		buf_writeptr = buf;
		while (lock_ref != NULL_REF && buf_available >= @.sizeof (lockrec_t)) {
			struct lock_desc *desc = &get_lock_desc (lock_ref);
			buf_writeptr@*lockrec_t->addr = get_desc_start (*desc);
			buf_writeptr@*lockrec_t->len = get_desc_end (*desc) - get_desc_start (*desc);
			buf_writeptr@*lockrec_t->mand = get_desc_mand (*desc);
			buf_writeptr@*lockrec_t->flags = get_desc_type (*desc);
			buf_writeptr = &(buf_writeptr@*lockrec_t->LASTFIELD);
			buf_available -= @.sizeof (lockrec_t);
			lock_ref = get_next_lock (lock_ref);
		}
		ok = TRUE;
		buf_len = buf_size - buf_available;
	}

	operation $gadrtranswaitdeletepadr {
		success_t ok;
		lock_mutex ();
		write_metainfo (MAKE_PTR (@phys_addr), @phys_len) => (@success, @phys_len);
		unlock_mutex ();
		@success = TRUE;
	}
