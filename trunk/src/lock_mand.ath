Author: Roland Niese
Copyright: Roland Niese
License: see files SOFTWARE-LICENSE, PATENT-LICENSE

context: pconf ulinux*,klinux*
context: cconf ulinux*,klinux*

brick #lock_mand

purpose Mandate-owned locks

desc
* Implements locks with an 'owner' id: mandates.
* Locks owned by the same operator do not block but melt together.
* Weaker (read) locks are overwritten by stronger (write) locks on melt (enstrengthment).
* Read locks do not conflict with each other on $lock, but write locks always conflict with any other lock.
* Pure address locks do not conflict with pure data locks and v.v.
* Forwards all operations except $lock and $unlock to the 'protected' resource on :<in.
* Does not generate own $lock or $unlock calls.
* Generates $retract operation calls on conflicting lock requests.
* granularity is 1.

      +--------+
      |        |
in ---+        +--- out
      |        |
      +--------+  
enddesc

static_header {
	#ifndef __LOCK_BRICK_STUFF__
	#define __LOCK_BRICK_STUFF__

#ifndef __KERNEL__	
	#include <pthread.h>
#else
	#include <linux/sched.h>
	#include <linux/wait.h>
	#include <asm/semaphore.h>
#endif

	// Abstract reference type to ease later conversion to Pointer Cache.
	typedef struct lock_rec *ref_t;
	// Essential properties of a lock.
	struct lock_desc {
		addr_t start;
		addr_t end;
		mand_t mandate;
		unsigned short flags;
	};
	// Record in a doubly linked list of locks.
	// Also contains implementation of thread blocking.
	struct lock_rec {
		ref_t next_start;
		ref_t prev_start;
		// Signal broadcasted when a locked block is [partially] unlocked.
#ifndef __KERNEL__
		pthread_cond_t unlocked;
#else
		wait_queue_head_t wait_queue;
#endif
		struct lock_desc desc;
	};
	// Filter for locks in the active list.
	typedef bool (*lock_test_t) (ref_t lock, struct lock_desc *desc);

	@.define LOCK_STORAGE_SIZE (1024)
	// Abstract reference values/querys/operations.
	@.define NULL_REF ((ref_t)NULL)
	
	// Abstract operations on the lock list.
	@.define has_next (REF) (REF->next_start != 0)
	@.define has_prev (REF) (REF->prev_end != 0)
	@.define next_start_from (REF) (REF->next_start)
	@.define prev_start_from (REF) (REF->prev_start)
	@.define next_end_from (REF) (REF->next_end)
	@.define prev_end_from (REF) (REF->prev_end)

	@.define get_desc_start (DESC) ((DESC).start)
	@.define get_desc_end (DESC) ((DESC).end)
	@.define get_desc_mand (DESC) ((DESC).mandate)
	@.define get_desc_type (DESC) ((DESC).flags & 0x000F)
	@.define get_desc_data_type (DESC) ((DESC).flags & 0x0003)
	@.define get_desc_addr_type (DESC) (((DESC).flags & 0x000C) >> 2)
	@.define set_desc_start (DESC, ADDR) ((DESC).start = (ADDR))
	@.define set_desc_end (DESC, ADDR) ((DESC).end = (ADDR))
	@.define set_desc_bounds (DESC, START_ADDR, END_ADDR) (set_desc_start (DESC, START_ADDR), set_desc_end (DESC, END_ADDR))
	@.define set_desc (DESC, ADDR_START, ADDR_END, MAND, TYPE) ((DESC).start = ADDR_START, (DESC).end = ADDR_END, (DESC).mandate = MAND, (DESC).flags = TYPE)
	
	@.define get_lock_start (REF) (get_desc_start ((REF)->desc))
	@.define get_lock_end (REF) (get_desc_end ((REF)->desc))
	@.define get_lock_mand (REF) (get_desc_mand ((REF)->desc))
	@.define get_lock_type (REF) (get_desc_type ((REF)->desc))
	@.define get_lock_data_type (REF) (get_desc_data_type ((REF)->desc))
	@.define get_lock_addr_type (REF) (get_desc_addr_type ((REF)->desc))
	@.define get_lock_desc (REF) ((REF)->desc)
	@.define set_lock_start (REF, ADDR) (set_desc_start ((REF)->desc, ADDR))
	@.define set_lock_end (REF, ADDR) (set_desc_end ((REF)->desc, ADDR))
	@.define set_lock_bounds (REF, START_ADDR, END_ADDR) (set_desc_bounds ((REF)->desc, START_ADDR, END_ADDR))
	@.define set_lock_mand (REF, MAND) (set_desc_mand ((REF)->desc, MAND))
	@.define set_lock_type (LOCK, LOCK_TYPE) ((LOCK)->desc.flags = (((LOCK)->desc.flags & ~0x000F) | (LOCK_TYPE)))	
	@.define set_lock_desc (LOCK, DESC) ((LOCK)->desc = (DESC))

	@.define make_lock_type (DATA_LOCK, ADDR_LOCK) (((ADDR_LOCK) << 2) | (DATA_LOCK))
	@.define is_stronger_or_equal (REF, DESC) (get_lock_data_type (REF) >= get_desc_data_type (DESC) && get_lock_addr_type (REF) >= get_desc_addr_type (DESC))
	@.define set_strongest_lock_type (REF, DESC) (set_lock_type (REF, make_lock_type (get_lock_data_type (REF) > get_desc_data_type (DESC) ? get_lock_data_type (REF) : get_desc_data_type (DESC), get_lock_addr_type (REF) > get_desc_addr_type (DESC) ? get_lock_addr_type (REF) : get_desc_addr_type (DESC))))

	#endif
}

static_data {
#ifndef __KERNEL__
	static pthread_cond_t null_cond;
#endif
	// lock_type = ((addr_lock << 2) + data_lock)
	// conflict <==> (lock_conflict_def[lock_type(A)] & (1 << lock_type(B))) != 0
	static unsigned short lock_conflict_def[] = {
		0x000, 0x444, 0x666, 0x000,
		0x700, 0x744, 0x766, 0x000,
		0x770, 0x774, 0x776
	};
	
	// Tests a lock record in a list for conflict with a given lock descriptor.
	// Used to find conflicting locks in $lock.
	// No need to test overlap, that is a precondition for the call of this filter.
	static bool is_conflict(ref_t lock, struct lock_desc *desc) {
		return (get_lock_mand (lock) != get_desc_mand (*desc) && lock_conflict_def[get_desc_type (*desc)] & (1 << get_lock_type (lock))) != 0;
	}
	
	// Tests a lock record in a list for 'ownage' by the same individual owning desc.
	// Used to find own locks in $lock (to melt them) and in $unlock (to split them).
	// No need to test overlap, that is a precondition for the call of this filter.
	static bool is_own (ref_t lock, struct lock_desc *desc) {
		return get_lock_mand (lock) == get_desc_mand (*desc);
	}
}

init {
#ifndef __KERNEL__
	memset(&null_cond, 0, sizeof null_cond);
#endif	
}

input :<in

	// Currently, this brick does not hold any data in this input nest, so $retract ops succeed.
	operation $retract {
		@success = TRUE;
	}

output :>out

	data {
		// Mutex to protect lock list from concurrent use. */
#ifndef __KERNEL__
		pthread_mutex_t lock_list_access;
#else
		DECLARE_MUTEX (mutex);
#endif
		// Storage for doubly linked list of locks. Could (should) later be stored in an input nest nest instead.
		struct lock_rec lock_rec_storage[LOCK_STORAGE_SIZE];
		// Reference to the lock with the lowest address.
		ref_t lock_starting_first;
		ref_t lock_starting_last;
		// Reference to the start of the list of unallocated blocks (first element has no 'previous' element).
		ref_t free_lock_recs;
	}

	// Allocates a new lock from the storage.
	@.define new_lock_rec () => (LOCK) {
		trace ("mand=%d allocate new lock record...\n", (int)@mandate);
		LOCK = @:>.free_lock_recs;
		@.fatal (!LOCK, "No more free lock records!");
		@:>.free_lock_recs = LOCK->next_start;
		#ifdef DEBUG
		LOCK->next_start = NULL_REF;
		LOCK->prev_start = NULL_REF;
		set_lock_bounds(LOCK, 0, 0);
		#endif
	}
	
	// Returns a lock record to the list of free records.
	// The record must have been removed from the active list (via remove_lock_rec) before!
	@.define free_lock_rec (LOCK) {
		trace ("mand=%d free lock [0x%llX..0x%llX]...\n", (int)@mandate, get_lock_start (LOCK), get_lock_end (LOCK));
		LOCK->next_start = @:>.free_lock_recs;
		LOCK->prev_start = NULL_REF;
		@:>.free_lock_recs = LOCK;
	}

	// Activates (creates condition) a lock record and adds it to the list.
	@.func add_lock_rec (ref_t lock) {
		ref_t scan;
		trace ("mand=%d activate lock [0x%llX..0x%llX]...\n", (int)@mandate, get_lock_start (lock), get_lock_end (lock));
		for (scan = @:>.lock_starting_first; scan != NULL_REF && get_lock_start (scan) < get_lock_start (lock); scan = scan->next_start);
#ifndef __KERNEL__		
// FIXME: pthread_cond_init if'ed as a workaround for the pthread_destroy bug (see FIXME below)
		if (memcmp(&lock->unlocked, &null_cond, sizeof lock->unlocked) == 0) {
			trace("mand=%d this is a fresh lock record. Creating new condition variable.\n", (int)@mandate);
			// This record's condition variable has not been init'ed yet.
			@.fatal (pthread_cond_init(&lock->unlocked, NULL) != 0, "Could not create POSIX lock!");
		}
		else {
			trace("mand=%d this lock record has been used before. Using the existing condition variable.\n", (int)@mandate);
		}
#else
		init_waitqueue_head (&lock->wait_queue);
#endif
		lock->next_start = scan;
		if (lock->next_start != NULL_REF) {
			lock->prev_start = lock->next_start->prev_start;
			lock->next_start->prev_start = lock;
		}
		else {
			lock->prev_start = @:>.lock_starting_last;
			@:>.lock_starting_last = lock;
		}
		if (lock->prev_start != NULL_REF) {
			lock->prev_start->next_start = lock;
		}
		else {
			@:>.lock_starting_first = lock;
		}		
	}
	
	// Deactivates (signals and destroys condition) and removes a lock record from the list.
	@.func remove_lock_rec (ref_t lock) {
		trace ("mand=%d deactivate lock [0x%llX..0x%llX]...\n", (int)@mandate, get_lock_start (lock), get_lock_end (lock));
		if (lock->prev_start != NULL_REF) {
			lock->prev_start->next_start = lock->next_start;
		}
		else {
			@:>.lock_starting_first = lock->next_start;
		}
		if (lock->next_start != NULL_REF) {
			lock->next_start->prev_start = lock->prev_start;
		}
		else {
			@:>.lock_starting_last = lock->prev_start;
		}
#ifndef __KERNEL__
		@.fatal (pthread_cond_broadcast(&lock->unlocked) != 0, "Could not release POSIX lock!");
// FIXME: (or better: fix pthreads)
//        pthread_cond_destroy blocks the caller if multiple threads were waiting
//        for the condition when it was broadcasted (after the broadcast,
//        no thread should be waiting for the condition anymore but for the
//        mutex instead, so it should be possible to destroy the condition variable.
//        I consider this a bug in pthreads.
//        So we don't destroy the condition but rather will reuse it later on.
//		@.fatal (pthread_cond_destroy(&lock->unlocked) != 0, "Could not destroy POSIX lock!");
#else
		wake_up_all (&lock->wait_queue);
#endif
	}

	// Allocates a new lock 'COPY' from the storage and copies
	// LOCK's essential characteristics to COPY. List and condition variables
	// are not copied.
	@.define copy_lock_rec (LOCK) => (COPY) {
		new_lock_rec () => (COPY);
		@.check ((COPY) != NULL_REF, "Out of lock records!");
		set_lock_desc (COPY, get_lock_desc (LOCK));
	}
	
	// Returns the first (=lowest start address) lock record in the list that
	// overlaps the specified descriptor and fulfills the specified requirement.
	@.define find_first_lock (desc_ptr, requirement) => (result) {
		find_next_lock_from (@:>.lock_starting_first, desc_ptr, requirement) => (result);
	}
	
	// Returns the next (=next-higher start address from 'lock') lock record in the list that
	// overlaps the specified descriptor and fulfills the specified requirement.
	@.func find_next_lock_from (ref_t lock, struct lock_desc *desc, lock_test_t requirement) => (ref_t result) {
		for (result = lock; result != NULL_REF; result = next_start_from (result)) {
			if (get_lock_end (result) > get_desc_start (*desc)) {
				if (get_lock_start (result) >= get_desc_end (*desc)) {
					result = NULL_REF;
					break;
				}
				if (requirement (result, desc)) {
					break;
				}
			}
		}
		if (result != NULL_REF) {
			trace ("mand=%d Found lock [0x%llX..0x%llX] in region [0x%llX..0x%llX].\n", (int)@mandate, get_lock_start (result), get_lock_end (result), get_desc_start (*desc), get_desc_end (*desc));
		}
		else {
			trace ("mand=%d Found no lock in region [0x%llX..0x%llX].\n", (int)@mandate, get_desc_start (*desc), get_desc_end (*desc));
		}
	}
	
	// Prints out the lock list. Debug function.
	@.func print_lock_list () {
		char buf[1024];
		int n;
		char *buf_end;
		ref_t lock, last_lock;
		lock = @:>.lock_starting_first;
		last_lock = NULL_REF;
		trace ("mand=%d lock list:\n", (int)@mandate);
		memset(buf, 0, sizeof buf);
		n = sizeof buf - 2;
		buf_end = buf;
		while (lock != NULL_REF && n > 0) {
			int len;
			snprintf(buf_end, n, "[0x%llX..0x%llX] ", get_lock_start (lock), get_lock_end (lock));
			len = strlen(buf_end);
			n -= len;
			buf_end += len;
			if (prev_start_from (lock) != last_lock) {
				trace ("mand=%d %s <-- INCONSISTENCY!!!\n", (int)@mandate, buf);
				return;
			}
			last_lock = lock;
			lock = next_start_from (lock);
		}
		if (@:>.lock_starting_last != last_lock) {
			trace ("mand=%d %s <-- FINAL INCONSISTENCY!!!\n", (int)@mandate, buf);
		}
		trace ("mand=%d %s (ok)\n", (int)@mandate, buf);
	}
	
	// Initializes/destroys status variables.
	// Stores status in brick variables. TODO: use a nest instead!
	operation $output_init {
		if (!@constr && @destr) {
#ifndef __KERNEL__
			pthread_mutex_destroy(&@:>.lock_list_access);
#endif
			@success = TRUE;
		}
		if (@constr && !@destr) {
			int i;
#ifndef __KERNEL__
			@.check(pthread_mutex_init(&@:>.lock_list_access, NULL) != 0, "Could not initialize lock_list_access mutex!");
#endif			
			
			memset(@:>.lock_rec_storage, 0, sizeof @:>.lock_rec_storage);
			for (i = 0; i < LOCK_STORAGE_SIZE - 1; i++) {
				@:>.lock_rec_storage[i].next_start = &@:>.lock_rec_storage[i + 1];
			}
			@:>.lock_rec_storage[i].next_start = NULL_REF;
			@:>.lock_starting_first = NULL_REF;
			@:>.lock_starting_last = NULL_REF;
			@:>.free_lock_recs = @:>.lock_rec_storage;
			@success = TRUE;
		}
	}

	// See description in brick header.
	operation $lock {
		addr_t obl_start = @log_addr;
		addr_t obl_end = @log_addr + @log_len;
		addr_t opt_start = @try_addr;
		addr_t opt_end = @try_addr + @try_len;
		struct lock_desc desc;
		ref_t conflictor;
		@.check (opt_start > obl_start || opt_end < obl_end, "Optional part [0x%llX..0x%llX] does not contain obligatory part [0x%llX..0x%llX]!", opt_start, opt_end, obl_start, obl_end);
		
		if (opt_end - opt_start == 0) {
			// Nothing to lock, return.
			trace ("mand=%d Requested lock [?0x%llX..[!0x%llX..0x%llX!]..0x%llX?] has no effect.\n", (int)@mandate, opt_start, obl_start, obl_end, opt_end);
			@success = TRUE;
			return;
		}
		
		set_desc (desc, obl_start, obl_end, @mandate, make_lock_type (@data_lock, @addr_lock));
		
#ifndef __KERNEL__		
		pthread_mutex_lock(&@:>.lock_list_access);
#else
		down(&mutex);
#endif
		trace ("mand=%d Lock [?0x%llX..[!0x%llX..0x%llX!]..0x%llX?] requested.\n", (int)@mandate, opt_start, obl_start, obl_end, opt_end);
		
		// Look for locks overlapping with the obligatory part of our requested lock.
		find_first_lock (&desc, is_conflict) => (conflictor);
		if (obl_end - obl_start > 0) {
			if (conflictor != NULL_REF) {
				if (@action != action_wait) {
					// There is a conflict and we are not supposed to wait for it, so break.
#ifndef __KERNEL__
					pthread_mutex_unlock(&@:>.lock_list_access);
#else
					up(&mutex);
#endif
					return;
				}
				do {
					// Wait for the lock to be released.
#ifndef __KERNEL__
					pthread_cond_wait(&conflictor->unlocked, &@:>.lock_list_access);
#else
					sleep_on(&lock->wait_queue);
#endif
					trace ("mand=%d got condition signal!\n", (int)@mandate);
					// Start all over again, the list has changed in the meantime
					// and 'conflictor' has become invalid.
					find_first_lock (&desc, is_conflict) => (conflictor);
				} while (conflictor != NULL_REF);
			}
		}
		else {
			if (conflictor != NULL_REF) {
				// This is the very special case of a zero-length obligatory part truly inside a conflicting lock,
				// What results in a successful lock of zero bytes.
				// (Code below relies on no locks across the whole obligatory part)
				@try_addr = @log_addr;
				@try_len = 0;
				@success = TRUE;
				trace ("Address 0x%llX truly contained inside lock [0x%llX..0x%llX], \"locked\" 0 bytes.\n", obl_start, get_lock_start (conflictor), get_lock_end (conflictor));
#ifndef __KERNEL__				
				pthread_mutex_unlock(&@:>.lock_list_access);
#else
				up(&mutex);
#endif
				return;
			}
		}
		
		// Cut the optional parts until nothing conflicts with them anymore.
		if (opt_start < obl_start || opt_end > obl_end) {
			set_desc_bounds (desc, opt_start, opt_end);
			find_first_lock (&desc, is_conflict) => (conflictor);
			while (conflictor != NULL_REF) {
				if (get_lock_end (conflictor) <= obl_start) {
					// Conflicting lock is below the obligatory part, cut optional part upwards.
					set_desc_start (desc, get_lock_end (conflictor));
				}
				else {
					// Conflicting is above the obligatory part, cut optional part downwards.
					set_desc_end (desc, get_lock_start (conflictor));
				}
				find_next_lock_from (conflictor, &desc, is_conflict) => (conflictor);
			}
		}
		
		// Now the way is free: add the new lock, if appropriate (lock length left > 0).
		if (get_desc_end (desc) > get_desc_start (desc) && @action != action_ask) {
			ref_t lock;
			// Split the new lock so that only the gaps between existing own locks are filled.
			// This is to prevent unnecessary wake-ups of operations waiting for those locks (what
			// would only result in an immediate new slept on the replacement lock).
			find_first_lock (&desc, is_own) => (lock);
			if (lock != NULL_REF) {
				if (get_lock_start (lock) < get_desc_start (desc)) {
					// The lock only partly overlaps the new lock. It may be necessary to split it.
					if (!is_stronger_or_equal (lock, desc)) {
						// The lock is (partly) weaker than the new lock and 
						// only the overlapping part has to be enstrengthened.
						ref_t new_lock;
						new_lock_rec () => (new_lock);
						@.check (new_lock == NULL_REF, "Out of lock records!");
						remove_lock_rec (lock);
						set_lock_desc (new_lock, get_lock_desc (lock));
						set_lock_end (new_lock, get_desc_start (desc));
						set_lock_start (lock, get_desc_start (desc));
						set_strongest_lock_type (lock, desc);
						add_lock_rec (lock);
						add_lock_rec (new_lock);
					}
					set_desc_start (desc, get_lock_end (lock));
				}
				while (get_desc_start (desc) < get_desc_end (desc)) {
					find_next_lock_from (lock, &desc, is_own) => (lock);
					// Loop invariant: there is no own lock between start of 'desc' and start of 'lock'.
					if (get_lock_start (lock) > get_desc_start (desc)) {
						// There is a gap to fill to the next existing lock.
						ref_t new_lock;
						new_lock_rec () => (new_lock);
						@.check (new_lock == NULL_REF, "Out of lock records!");
						set_lock_desc (new_lock, desc);
						set_lock_end (new_lock, get_lock_start (lock));
						add_lock_rec (new_lock);						
					}
					if (get_lock_end (lock) > get_desc_end (desc)) {
						// The existing lock exceeds the new lock and might need to be split.
						if (!is_stronger_or_equal (lock, desc)) {
							// It is (partly) weaker, and the overlapping part needs
							// enstrengthment, so split it.
							ref_t new_lock;
							new_lock_rec () => (new_lock);
							@.check (new_lock == NULL_REF, "Out of lock records!");
							remove_lock_rec (lock);
							set_lock_desc (new_lock, get_lock_desc (lock));
							set_lock_start (new_lock, get_desc_end (desc));
							set_lock_end (lock, get_desc_end (desc));
							add_lock_rec (lock);
							add_lock_rec (new_lock);
						}
					}
					set_strongest_lock_type (lock, desc);
					set_desc_start (desc, get_lock_end (lock));
				}
			}
		}

		trace ("mand=%d Lock [0x%llX..0x%llX] granted.\n", (int)@mandate, get_desc_start (desc), get_desc_end (desc));
		print_lock_list ();
#ifndef __KERNEL__		
		pthread_mutex_unlock(&@:>.lock_list_access);
#else
		up(&mutex);
#endif
		
		@try_addr = get_desc_start (desc);
		@try_len = get_desc_end (desc) - get_desc_start (desc);
		@success = TRUE;
	}
	
	// See description in brick header.
	operation $unlock {
		ref_t lock;
		struct lock_desc desc;
		// Unlocking will always succeed, so we unlock the whole optional part right away.
		set_desc (desc, @try_addr, @try_addr + @try_len, @mandate, make_lock_type (lock_write, lock_write));
		@.check (get_desc_start (desc) > @log_addr || get_desc_end (desc) < @log_addr + @log_len, "Optional part [0x%llX..0x%llX] does not contain obligatory part [0x%llX..0x%llX]!", get_desc_start (desc), get_desc_end (desc), @log_addr, @log_addr + @log_len);

#ifndef __KERNEL__
		pthread_mutex_lock(&@:>.lock_list_access);
#else
		down(&mutex);
#endif
		trace ("mand=%d Unlock of [0x%llX..0x%llX] requested.\n", (int)@mandate, get_desc_start (desc), get_desc_end (desc));
		
		find_first_lock (&desc, is_own) => (lock);
		while (lock != NULL_REF) {
			ref_t next_lock = next_start_from (lock);
			// If the block is not completely covered by this $unlock, it must be split.
			// Create the (potential) lower and the upper leftovers as separate new locks.
			if (get_lock_start (lock) < get_desc_start (desc)) {
				ref_t lower_part;
				copy_lock_rec (lock) => (lower_part);
				set_lock_end (lower_part, get_desc_start (desc));
				add_lock_rec (lower_part);
			}
			if (get_lock_end (lock) > get_desc_end (desc)) {
				ref_t upper_part;
				copy_lock_rec (lock) => (upper_part);
				set_lock_start (upper_part, get_desc_end (desc));
				add_lock_rec (upper_part);
			}
			// Now remove the original lock. 
			remove_lock_rec (lock);
			free_lock_rec (lock);
			find_next_lock_from (next_lock, &desc, is_own) => (lock);
		}
		trace ("mand=%d Lock [0x%llX..0x%llX] released.\n", (int)@mandate, get_desc_start (desc), get_desc_end (desc));
		print_lock_list ();
#ifndef __KERNEL__		
		pthread_mutex_unlock(&@:>.lock_list_access);
#else
		up(&mutex);
#endif
		@success = TRUE;
	}

	// Forward any other operations to :<in.
	operation $op {
		@=outputcall :<in$op @args;
	}
